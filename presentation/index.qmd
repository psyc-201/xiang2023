---
title: "Thinking About Collaborators"
subtitle: "Replication of Experiment 1 in @Xiang2023-zo"
author:
  - name: Jacob C. Zimmerman
    orcid: 0000-0002-6010-8086
    email: j2zimmerman@ucsd.edu
    affiliations: UC San Diego
date: last-modified
filters:
  - highlight-text
  - style-speaker-note
format:
  clean-revealjs:
    incremental: true
    speaker_notes_style: "_extensions/stefanocoretta/tachyonsextra/tachyonsextra.css"
tbl-cap-location: bottom
execute:
  echo: false
  freeze: auto
bibliography:
  - "../references.bib"
csl: "../apa.csl"
---

```{r}
#| include: false
library(tidyverse)
attach("./writeup/results/.RData")
```

## Collaboration is fundamental to humans

- Fundamental to human success is how we've evolved to collaborate effectively [@Tomasello2005]
- Collaboration requires judgments of what potential collaborators could bring to the table, and how much effort they will allocate
- How complex are these judgments?

::: {.notes}
Collaboration is fundamental to humans

Fundamental to human success is how we've evolved to collaborate effectively. There are many cognitive processes involved in this.

For example, in order to make decisions about who to work with, we make judgments of what potential collaborators could bring to the table, and how much effort they will allocate

As usual, evolution prefers the simplest solution to problems: so how complex is our solution?
:::

## How do we judge what others could bring to the table?

- We observe them work, to update our beliefs about them (Bayesian inference) [@Premack_Woodruff_1978]
  - Belief: how _competent_ they are [@Jara-Ettinger2015]
  - Belief: how much _effort_ they are putting in, given their competence and how much incentive there is [@Jara-Ettinger2016]

::: {.notes}
So how do we judge what others could bring to the table?

Well, we observe them working to update our beliefs about them; Bayesian inference is a good candidate for how we adjust our beliefs given observations of the world

Some of our beliefs are about how competent they are

and how much effort they are putting in

::: {.box-tip}
While we also infer the difficulty of the task, @Xiang2023-zo is seeking to integrate the independent role of perceived competence.
:::
:::

## How do we judge what others could bring to the table, and _how much effort they will allocate_?

- Simple models of effort allocation: we could think that...
  - _Social compensation_: people assume others won't apply any effort, so they allocate effort as though they were working alone [@Williams1991-sv] ([solitary model]{color="navy"})
  - _Social loafing_: people assume others will apply maximum effort, so they allocate only enough effort to help them [@Karau_Williams_1993; @Latane1979] ([compensatory model]{color="blue"})

- @Xiang2023-zo propose a more complex model: we think that...
  - _Jointly optimize effort_: people infer how much effort others will allocate and _how much the others think they will allocate_, and then calibrate their effort accordingly -- seeking to optimize their _combined effort_ ([joint effort model]{color="red"})

::: {.notes}
After we infer each other's competence and effort from observing their behavior, how do we judge how they much effort they will allocate?

There are a couple simple solutions that evolution could have chosen for how we calibrate our own effort -> &lt;read slide&gt;

"Optimize combined effort" means calibrate the combined effort to be enough to achieve the task (and for effort also not to be unfairly allocated)

::: {.box-tip}
We make some assumptions to make the inferences in the proposed model tractable [@Jara-Ettinger2016]:
  1. If they succeed, they must have at least _some_ competence and have put _some_ effort in
  2. If they are competent enough to succeed, they will allocate the minimum effort needed given their competence
  3. Effort is costly, so if they succeed with low incentive, they must really _desire_ the reward (i.e., reward has high utility to them)
  4. They discount unequal effort allocation (i.e., they care somewhat about fairness)
  5. (Practical assumption) Competence does not change across observations
  6. (Practical assumption) Competence and effort each have a defined range (0% to 100%)

**Illustrating the basic framework of inferences:**

We infer a person's competence _and_ effort _simultaneously_ by internally modeling their mutual contingencies, and the influence of incentive:

1. Effort turns competence into success
  - e.g., if they succeed, they must have some competence and have put some effort in
  - **Success means combined competence and effort is at least X**
2. They don't put in more effort than necessary, given their competence
  - e.g., if they are competent enough to succeed, they allocate the minimum effort needed given their competence
  - **Success given their competence means at most Y effort**
3. Effort is motivated by _incentive_
  - e.g., effort is costly, so if they succeed with low incentive, they must really _desire_ the reward (i.e., reward has high utility to them)
  - **Success means at least Z desire for the reward and the minimum W effort that this motivates**
:::
:::

## @Xiang2023-zo: Lift That Box!

- @Xiang2023-zo used a game-like task to collect participants' judgments of competence, effort, and likelihood of success of virtual collaborators, in order to see how well the [joint effort model]{color="red"} predicted these judgments
  <iframe src="https://psyc-201.github.io/xiang2023/exp/index1.html?TEST" style="zoom: .75; width: 133%; height: 20em"></iframe>

::: {.notes}
The authors then sought to validate their more complex model

&lt;Read slide. Then demo the task and point out the rounds 1-2 are individual, and round 3 is joint activity&gt;

So they had many different results, but let's take a look at the key ones I was interested in replicating
:::

## @Xiang2023-zo: Results

```{r}
#| include: false
set.seed(1)
fig3c_behavioralonly <- original.dat %>%
  ggplot(aes(scenario, prob, group = model, color = model)) +

  stat_summary(data = original.dat %>% filter(round == 3 & agent == 'A'), fun.data = 'mean_cl_boot', geom = 'errorbar', width = .1) +
  stat_summary(data = original.dat %>% filter(round == 3 & agent == 'A'), fun = 'mean', geom = 'point', size = 1.5) +

  scale_color_manual(
    name = NULL,
    labels = c('Data'),
    values = c('#000000'),
    limits = c('data'),
    guide = ggh4x::guide_stringlegend(position = "top")
  ) +
  theme_minimal() + 
  theme(legend.text = element_text(face = "bold.italic", size = 18 / .pt)) +

  coord_cartesian(ylim = c(0, 100)) +
  labs(x='Round 1 and Round 2 outcome', y='Lift probability in Round 3 (%)', color=NULL)

# write temporarily to file to render in qmd
ggsave('./presentation/fig3c_behavioral_only.png', plot = fig3c_behavioralonly, width=4, height=3)
```

1. Participants thought that collaborators can succeed together even if they had always failed alone (`r papaja::apa_print(original.ttest_result)$statistic`; $d = `r papaja::apa_num(original.cohensd$Cohens_d)`$)
2. Participants also thought that **collaborators' chance of succeeding together increased with more individual successes** (i.e., collaborators inferred each other's competence and effort from prior attempts)
3. **Only the proposed [joint effort model]{color="red"} predicted both:**

::: {.fragment style="font-size: 60%; text-align: center"}
![(Preview) Notice `F,F;F,F`, the left-to-right trend, and that [joint effort]{color="red"} fits the best.](../writeup/original-fig3c.png){height=250 style="--r-block-margin: 0"}
:::

::: {.notes}
&lt;Read slide&gt;

This evidence suggests that their model is worth the complexity!

::: {.box-tip}
Also, they found that the increase was **gradual** (i.e., collaborators tried to distribute effort fairly)
:::
:::

## Replicating @Xiang2023-zo: Power

- I estimated that we needed only 9 participants (as this would give us `r papaja::apa_num(pwr::pwr.t.test(n=9, d=original.cohensd$Cohens_d, sig.level=.05, type="one.sample", alternative="two.sided")$power * 100)`% power for the main effect)
- However, to replicate the qualitative pattern, in the absence of a good expectation of how many participants were necessary for this, I chose to collect the full 50 participants
  - As a side-effect, this will also enable future exploratory analysis of the dataset, which I'll briefly describe later

::: {.notes}
&lt;Read slide&gt;
:::

## Replicating @Xiang2023-zo: Modeling

<!-- - I expected the basic effect to replicate due the very large effect size
- 80% confident the qualitative pattern would replicate -->

- For the computational model, I chose to reimplement the model in `memo`, a new probabilistic programming language [@chandra2025memo] for performance and extensibility advantages (for future work) 
  - Needed to validate that new model had comparable results
  - Worked through distinguishing trivial implementation differences from implementation mistakes
  - Accomplished this by changing one implementation variable at a time and measuring its impact to determine where discrepancies came from

::: {.notes}
I expected the basic effect to replicate since there's high signal to noise in the measure (high t-value and low SE),

and I was 4/5 confident the qualitative pattern would replicate.

&lt;Read slide&gt;

::: {.box-tip}
Worked through distinguishing trivial implementation differences (e.g., going from a continuous prior to a discrete prior) from meaningful implementation discrepancies (e.g., errors in the model implementation)

How I changed one variable at a time in modeling: webppl MCMC -> webppl enumerate -> memo enumerate
:::
:::

## Replication Results: Confirmatory

```{r}
#| include: false
set.seed(1)
replication.fig3c_behavioralonly <- replication.dat %>%
  ggplot(aes(scenario, prob, group = model, color = model)) +

  stat_summary(data = replication.dat %>% filter(round == 3 & agent == 'A'), fun.data = 'mean_cl_boot', geom = 'errorbar', width = .1) +
  stat_summary(data = replication.dat %>% filter(round == 3 & agent == 'A'), fun = 'mean', geom = 'point', size = 1.5) +

  scale_color_manual(
    name = NULL,
    labels = c('Data'),
    values = c('#000000'),
    limits = c('data'),
    guide = ggh4x::guide_stringlegend(position = "top")
  ) +
  theme_minimal() + 
  theme(legend.text = element_text(face = "bold.italic", size = 18 / .pt)) +

  coord_cartesian(ylim = c(0, 100)) +
  labs(x='Round 1 and Round 2 outcome', y='Lift probability in Round 3 (%)', color=NULL)

# write temporarily to file to render in qmd
ggsave('./presentation/replication-fig3c_behavioral_only.png', plot = replication.fig3c_behavioralonly, width=4, height=3)
```

:::: {.columns}

::: {.column .fragment}
![Figure 3C (Behavioral Only). Error bars show bootstrapped 95% CIs.](./fig3c_behavioral_only.png)

::: {.fragment}
Collaborators can succeed together even if they have failed alone (`r papaja::apa_print(original.ttest_result)$statistic`; $d = `r papaja::apa_num(original.cohensd$Cohens_d)`$)
:::
:::

::: {.column .fragment}
![Replication (Behavioral Only). Error bars show bootstrapped 95% CIs.](./replication-fig3c_behavioral_only.png)

::: {.fragment}
**Replicated!** (`r papaja::apa_print(replication.ttest_result)$statistic`; $d = `r papaja::apa_num(replication.cohensd$Cohens_d)`$)
:::
:::

::::

::: {.notes}
&lt;Read slide, explaining plots&gt;
:::

## Replication Results: Exploratory

:::: {.columns}

::: {.column .fragment}
![Figure 3C. Model simulations averaged across 10 iterations. Error bars show bootstrapped 95% CIs.](../writeup/original-fig3c.png)

Collaborators' chance of succeeding together increased with more individual success, validating [joint effort model]{color="red"}
:::

::: {.column .fragment}
![Replication. Model simulations use strengths in steps of .03. Error bars show bootstrapped 95% CIs.](../writeup/replication-fig3c.png)

**Replicated!**
:::

::::

::: {.notes}
&lt;Read slide, explaining plots&gt;

In sum, they replcicated that joint success is possible even if there is no prior evidence of individual success, AND joint success likelihood increases as cases of individual success increases. In conjunction, these results support the [joint effort model]{color="red"} of reasoning about collaborators' effort
:::

## Discussion

- Replication success was expected due to large effect size and clear pattern
- I observed slight differences:
  - Larger CIs
  - Flat section of the curve (i.e., similarity between two scenarios) not seen before
  - Effect size (_d_) was lower than original, as should be expected

::: {.notes}
&lt;Read slide&gt;

::: {.box-tip}
Flat part could be explained by the safe joint effort model (can show the figure that includes the safe joint effort model)
:::

:::

## Discussion: Next Steps

- To validate the [joint effort model]{color="red"} further, I will quantify the model fits (and confirm with new data), modeling within-participant variability independently from variability due to scenario (outcomes of rounds 1 and 2)

::: {.fragment}
![Figure 3C Replication with participant-level data.](../writeup/replication-fig3c-withrawdata.png){height=430}
:::

::: {.notes}
Now some next steps...

&lt;Read slide&gt;

We can see that there may be variance or even heterogeneity across participants that could be modeled; I'd like to estimate model fits, explicitly modeling for the between-participants variability, using model-likelihood measures for each participant then aggregating across participants.

Also, Maybe the greater variance in the data (driving the larger CIs) is due to this heterogeneity that might not have been present in the original data. Or it could just be due to publication bias/effect inflation.

::: {.box-tip}
Participant-level variability can be seen as amplified, diminished, or biased probability judgments across scenarios that could be attributed to the participant rather than to the scenarios; we see that there could be a lot of this variability by looking at the underlying participant-level data.

One way I might quantify the model fits with both types of variability is to estimate the likelihood of each participant's data under each model then summarizing across participants to estimate the likelihood of each model.
:::
:::

## Discussion: Task

- Some limitations:
  - The task is not very naturalistic (limits ecological validity)
  - The participant isn't a collaborator themselves (limits construct validity)
- Future steps:
  - Rich textual vignettes that paint a fuller scene than just stick figures before and after they attempt to lift boxes
  - Include the participant as a collaborator, e.g., with their own incentive related to the virtual collaborators' outcome

::: {.notes}
Last,

I want to acknowledge describe some limitations of the task -> &lt;read slide&gt;
:::

## Thank you!

::: {style="text-align: center; font-size: 300%"}
Questions?
:::

## References

::: {#refs}
:::
