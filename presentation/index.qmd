---
title: "Thinking About Collaborators"
subtitle: "Replication of Experiment 1 in @Xiang2023-zo"
author:
  - name: Jacob C. Zimmerman
    orcid: 0000-0002-6010-8086
    email: j2zimmerman@ucsd.edu
    affiliations: UC San Diego
date: last-modified
format:
  clean-revealjs:
    incremental: true
execute:
  echo: false
  freeze: auto
bibliography:
  - "references.bib"
csl: "apa.csl"
---

```{r}
#| include: false
library(tidyverse)
attach("./writeup/results/.RData")
```

## How do we judge what others could bring to the table?

<!-- TODO: cite the claims; see original paper for citations -->

- We observe them work, to update our beliefs! (Bayesian)
  - Belief: how _competent_ they are (cite)
  - Belief: how much _effort_ they are putting in (ToM), given their competence (cite) and how much incentive there is (cite)

- We make assumptions:
  1. If they succeed, they must have some competence and have put some effort in
  2. If they are competent enough to succeed, they will allocate the minimum effort needed given their competence
  3. Effort is costly, so if they succeed with low incentive, they must really _desire_ the reward (i.e., reward has high utility to them)

::: {.notes}
Collaboration is key to human success. When we think about other people we could work with, we need to judge who's capable enough to be a good potential collaborator. So how do we make these judgments?

We observe them work to update our beliefs; Bayesian inference is a good candidate for how we adjust our beliefs given observations of the world

We can make these inferences tractable by making some assumptions:
:::

## When working together, how do we calibrate our own effort?

- Simple models:
  - We simply think the other person won't apply any effort, so we allocate effort like we are working alone ("solitary" model)
  - We simply think the other person will apply maximum effort, so we allocate only enough effort to help them ("compensatory" model)

- Proposed model in @Xiang2023-zo:
  - If we think they can succeed, we _jointly_ optimize our effort: we _recursively_ infer how much effort the other will allocate, and adjust our own effort accordingly, while also trying to be fair ("joint effort" model)

::: {.notes}
After we infer each other's competence from observing their behavior, how do we set our own effort when collaborating with them?
:::

## @Xiang2023-zo: Lift That Box!

- @Xiang2023-zo used a game-like task to collect participants' judgments of competence, effort, and likelihood of success, to test the "joint effort" model
  <iframe src="https://psyc-201.github.io/xiang2023/exp/index1.html?TEST" width=100% style="height: 15em"></iframe>

::: {.notes}
The original study used this task which looks like... (demo... mention joint and individual activity)
:::

## @Xiang2023-zo: Lift That Box!

- Some critiques:
  - The task is not very naturalistic (limits ecological validity)
  - The participant isn't a collaborator themselves (limits construct validity)

## @Xiang2023-zo: Result

```{r}
#| include: false
fig3c_behavioralonly <- original.dat %>%
  ggplot(aes(scenario, prob, group = model, color = model)) +

  stat_summary(data = original.dat %>% filter(round == 3 & agent == 'A'), fun.data = 'mean_cl_boot', geom = 'errorbar', width = .1) +
  stat_summary(data = original.dat %>% filter(round == 3 & agent == 'A'), fun = 'mean', geom = 'point', size = 1.5) +

  scale_color_manual(name = NULL, labels = c('Data'),
                               values = c('#000000'),
                               limits = c('data')) +
  theme_bw() + 
  theme(legend.position="none") +
  coord_cartesian(ylim = c(0, 100)) +
  labs(x='Round 1 and Round 2 outcome', y='Lift probability in Round 3 (%)', color=NULL)

# write temporarily to file to render in qmd
ggsave('./presentation/fig3c_behavioral_only.png', plot = fig3c_behavioralonly, width=4, height=3)
```

- Ps thought that collaborators could achieve a task jointly even if they have previously failed the task individually (_t_(`r original.ttest_result$parameter`) = `r original.ttest_result$statistic`, _p_ = `r signif(original.ttest_result$p.value, 2)`; **_d_ = `r round(original.cohensd$Cohens_d,2)`**)
- Ps thought that collaborators' chance of jointly achieving the task gradually increased as collaborators succeeded more individually (i.e., that collaborators thought about each other's effort and tried to distribute effort fairly)  
  ![](./fig3c_behavioral_only.png){height=200}
- Their proposed "joint effort" model was the only one that explained both effects  
  ![](../writeup/figure3c.png){height=200}
  ![](../writeup/figure3c-legend.png){height=100}

::: {.notes}
can note that there's a green dot as well showing maximal model, but it's not relevant to us - it's another alternative model that didn't give predictions for most scenarios
:::

## Replicating @Xiang2023-zo

- I estimated that we needed only 9 participants (as this would give us `r pwr::pwr.t.test(n=9, d=original.cohensd$Cohens_d, sig.level=.05, type="one.sample", alternative="two.sided")$power * 100`% power for the main effect)
- However, to replicate the qualitative pattern, in the absence of a good expectation, I wanted the full 50 participants

## Replicating @Xiang2023-zo: Statistics and Modeling

- I expected the basic effect to replicate

- For the computational model, I chose to reimplement the model in a new probabilistic programming language (memo) for performance and extensibility advantages (for future work) 
  - Needed to validate that new model had comparable results
  - Worked through distinguishing trivial implementation differences (e.g., going from a continuous prior to a discrete prior) from meaningful implementation discrepancies (e.g., errors in the model implementation)
  - Overcame this by changing one implementation variable at a time and measuring its impact to characterize where discrepancies came from (to qualify how they looked numerically)

::: {.notes}
I expected the basic effect to replicate since there's high signal to noise in the measure (high t-value and low SE), and I was 4/5 confident the qualitative pattern would replicate.

(how I changed one variable at a time in modeling: webppl MCMC -> webppl enumerate -> memo enumerate)
:::

## Replication Results

```{r}
#| include: false
replication.fig3c_behavioralonly <- replication.dat %>%
  ggplot(aes(scenario, prob, group = model, color = model)) +

  stat_summary(data = replication.dat %>% filter(round == 3 & agent == 'A'), fun.data = 'mean_cl_boot', geom = 'errorbar', width = .1) +
  stat_summary(data = replication.dat %>% filter(round == 3 & agent == 'A'), fun = 'mean', geom = 'point', size = 1.5) +

  scale_color_manual(name = NULL, labels = c('Data'),
                               values = c('#000000'),
                               limits = c('data')) +
  theme_bw() + 
  theme(legend.position="none") +
  coord_cartesian(ylim = c(0, 100)) +
  labs(x='Round 1 and Round 2 outcome', y='Lift probability in Round 3 (%)', color=NULL)

# write temporarily to file to render in qmd
ggsave('./presentation/replication-fig3c_behavioral_only.png', plot = replication.fig3c_behavioralonly, width=4, height=3)
```

Original | **Replication**
---- | ----
![Figure 3C (Behavioral Only)](./fig3c_behavioral_only.png)<br><br>"Note. ... Error bars indicate bootstrapped 95% confidence intervals." (p. 1571) | ![Figure 3C Replication (Behavioral Only)](./replication-fig3c_behavioral_only.png)<br><br>Note. Error bars indicate bootstrapped 95% confidence intervals.
Collaborators could achieve a task jointly even if they have previously failed the task individually (_t_(`r original.ttest_result$parameter`) = `r original.ttest_result$statistic`, _p_ = `r signif(original.ttest_result$p.value, 2)`; **_d_ = `r round(original.cohensd$Cohens_d,2)`**) | **Replicated!** (_t_(`r replication.ttest_result$parameter`) = `r replication.ttest_result$statistic`, _p_ = `r signif(replication.ttest_result$p.value, 2)`; **_d_ = `r round(replication.cohensd$Cohens_d,2)`**)

- Replicated: joint success is possible even if there is no prior evidence of success

## Replication Results

<!-- TODO: style better; include legend in plots themselves --> 

Original | **Replication**
---- | ----
![](../writeup/figure3c.png) ![](../writeup/figure3c-legend.png){height=150}<br><br>"Note. ... Model simulations averaged over 10 runs. Error bars indicate bootstrapped 95% confidence intervals." (p. 1571) | ![Figure 3C Replication](../writeup/replication-fig3c.png) ![](../writeup/figure3c-legend.png){height=150}<br><br>Note. Model simulations consider strengths from 1 to 10 (inclusive) in steps of 0.03 (determined by memory constraints). Error bars indicate bootstrapped 95% confidence intervals.
Joint success likelihood increases as cases of individual success increases | **Replicated!**

- Replicated: joint success is possible even if there is no prior evidence of success, AND **joint success likelihood increases as cases of individual success increases**
  - In conjunction, these support the joint-effort model of reasoning about collaborators' competence and effort.

<!-- TODO: try to make the conclusion clearer - could make the language here mirror the language in the introduction -->

## Discussion

- Replication success was expected due to large effect size and clear pattern
- I observed slight differences:
  - larger CIs
  - a flat section of the curve (i.e. similarity between two scenarios) not seen before; could be explained by the safe joint effort model (can show the figure that includes the safe joint effort model)
  - Effect size (_d_) was lower than original, as should be expected

::: {.notes}
Maybe the larger CIs are due to more variance than in the original?
:::

## Discussion

- To test the hypotheses further, I might model within-subject variability independent from scenario condition

<!-- TODO: show image of the fig3c replication with raw data -->

::: {.notes}
Maybe the greater variance in the data (driving the larger CI values) is due to this heterogeneity that might not have been present in the original data
:::

## References

::: {#refs}
:::

<!-- ## (Extra) The basic framework: inferences

We infer a person's competence _and_ effort _simultaneously_ by internally modeling their mutual contingencies, and the influence of incentive:

1. Effort turns competence into success
  - e.g., if they succeed, they must have some competence and have put some effort in
  - **Success means combined competence and effort is at least X**
2. They don't put in more effort than necessary, given their competence
  - e.g., if they are competent enough to succeed, they allocate the minimum effort needed given their competence
  - **Success given their competence means at most Y effort**
3. Effort is motivated by _incentive_
  - e.g., effort is costly, so if they succeed with low incentive, they must really _desire_ the reward (i.e., reward has high utility to them)
  - **Success means at least Z desire for the reward and the minimum W effort that this motivates**

::: {.notes}
Also:
- (Practical assumption) Competence does not change across observations
- (Practical assumption) Competence and effort each have a defined range (0% to 100%)
::: -->
