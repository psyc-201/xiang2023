---
title: "Thinking About Collaborators"
subtitle: "Replication of Experiment 1 in @Xiang2023-zo"
author:
  - name: Jacob C. Zimmerman
    orcid: 0000-0002-6010-8086
    email: j2zimmerman@ucsd.edu
    affiliations: UC San Diego
date: last-modified
format:
  clean-revealjs:
    incremental: true
tbl-cap-location: bottom
execute:
  echo: false
  freeze: auto
bibliography:
  - "references.bib"
csl: "apa.csl"
---

```{r}
#| include: false
library(tidyverse)
attach("./writeup/results/.RData")
```

## How do we judge what others could bring to the table?

- We observe them work, to update our beliefs! (Bayesian inference) [@Premack_Woodruff_1978]
  - Belief: how _competent_ they are [@Jara-Ettinger2015]
  - Belief: how much _effort_ they are putting in, given their competence and how much incentive there is [@Jara-Ettinger2016]

- We make assumptions [@Jara-Ettinger2016]:
  1. If they succeed, they must have at least _some_ competence and have put _some_ effort in
  2. If they are competent enough to succeed, they will allocate the minimum effort needed given their competence
  3. Effort is costly, so if they succeed with low incentive, they must really _desire_ the reward (i.e., reward has high utility to them)

::: {.notes}
Collaboration is key to human success [@Tomasello2005]. When we think about other people we could work with, we need to judge who's capable enough to be a good potential collaborator. So how do we make these judgments?

We observe them work to update our beliefs; Bayesian inference is a good candidate for how we adjust our beliefs given observations of the world

Note: while we also infer the difficulty of the task @Xiang2023-zo is seeking to integrate the independent role of perceived competence.

We can make these inferences tractable by making some assumptions... (next slide)
:::

## When working together, how do we calibrate our own effort?

- Simple models:
  - _Social compensation_: we simply think the other person won't apply any effort, so we allocate effort like we are working alone [@Williams1991-sv] ("solitary" model)
  - _Social loafing_: we simply think the other person will apply maximum effort, so we allocate only enough effort to help them [@Karau_Williams_1993; @Latane1979] ("compensatory" model)

- Proposed model in @Xiang2023-zo:
  - _Jointly optimize effort_: we infer how much effort the other will allocate and how much they think _we_ will allocate, and we adjust our own effort accordingly, while trying to be fair ("joint effort" model)

::: {.notes}
After we infer each other's competence from observing their behavior, how do we set our own effort when collaborating with them?
:::

## @Xiang2023-zo: Lift That Box!

- @Xiang2023-zo used a game-like task to collect participants' judgments of competence, effort, and likelihood of success, to see how well the "joint effort" model predicted them
  <iframe src="https://psyc-201.github.io/xiang2023/exp/index1.html?TEST" width=100% style="height: 15em"></iframe>

::: {.notes}
The original study used this task which looks like... (demo... mention joint and individual activity)
:::

## @Xiang2023-zo: Lift That Box!

- Some critiques:
  - The task is not very naturalistic (limits ecological validity)
  - The participant isn't a collaborator themselves (limits construct validity)

## @Xiang2023-zo: Result

```{r}
#| include: false
set.seed(1)
fig3c_behavioralonly <- original.dat %>%
  ggplot(aes(scenario, prob, group = model, color = model)) +

  stat_summary(data = original.dat %>% filter(round == 3 & agent == 'A'), fun.data = 'mean_cl_boot', geom = 'errorbar', width = .1) +
  stat_summary(data = original.dat %>% filter(round == 3 & agent == 'A'), fun = 'mean', geom = 'point', size = 1.5) +

  scale_color_manual(
    name = NULL,
    labels = c('Data'),
    values = c('#000000'),
    limits = c('data'),
    guide = ggh4x::guide_stringlegend(position = "top")
  ) +
  theme_minimal() + 
  theme(legend.text = element_text(face = "bold.italic", size = 18 / .pt)) +

  coord_cartesian(ylim = c(0, 100)) +
  labs(x='Round 1 and Round 2 outcome', y='Lift probability in Round 3 (%)', color=NULL)

# write temporarily to file to render in qmd
ggsave('./presentation/fig3c_behavioral_only.png', plot = fig3c_behavioralonly, width=4, height=3)
```

- Participants thought that collaborators can succeed together even if they have failed alone (`r papaja::apa_print(original.ttest_result)$statistic`; $d = `r papaja::apa_num(original.cohensd$Cohens_d)`$)
- They also thought that collaborators' chance of succeeding together increased with more individual successes (i.e., that collaborators inferred each other's effort from prior attempts) and this was gradual (i.e., collaborators tried to distribute effort fairly)
- **Only the proposed "joint effort" model predicted both:**  
  ![](../writeup/original-fig3c.png){height=250}

::: {.notes}
can note that there's a green dot as well showing maximal model, but it's not relevant to us - it's another alternative model that didn't give predictions for most scenarios
:::

## Replicating @Xiang2023-zo

- I estimated that we needed only 9 participants (as this would give us `r papaja::apa_num(pwr::pwr.t.test(n=9, d=original.cohensd$Cohens_d, sig.level=.05, type="one.sample", alternative="two.sided")$power * 100)`% power for the main effect)
- However, to replicate the qualitative pattern, in the absence of a good expectation, I wanted the full 50 participants

## Replicating @Xiang2023-zo: Statistics and Modeling

<!-- TODO: cite memo -->

- I expected the basic effect to replicate

- For the computational model, I chose to reimplement the model in a new probabilistic programming language (memo) for performance and extensibility advantages (for future work) 
  - Needed to validate that new model had comparable results
  - Worked through distinguishing trivial implementation differences (e.g., going from a continuous prior to a discrete prior) from meaningful implementation discrepancies (e.g., errors in the model implementation)
  - Overcame this by changing one implementation variable at a time and measuring its impact to characterize where discrepancies came from (to qualify how they looked numerically)

::: {.notes}
I expected the basic effect to replicate since there's high signal to noise in the measure (high t-value and low SE), and I was 4/5 confident the qualitative pattern would replicate.

(how I changed one variable at a time in modeling: webppl MCMC -> webppl enumerate -> memo enumerate)
:::

## Replication Results: Confirmatory

```{r}
#| include: false
set.seed(1)
replication.fig3c_behavioralonly <- replication.dat %>%
  ggplot(aes(scenario, prob, group = model, color = model)) +

  stat_summary(data = replication.dat %>% filter(round == 3 & agent == 'A'), fun.data = 'mean_cl_boot', geom = 'errorbar', width = .1) +
  stat_summary(data = replication.dat %>% filter(round == 3 & agent == 'A'), fun = 'mean', geom = 'point', size = 1.5) +

  scale_color_manual(
    name = NULL,
    labels = c('Data'),
    values = c('#000000'),
    limits = c('data'),
    guide = ggh4x::guide_stringlegend(position = "top")
  ) +
  theme_minimal() + 
  theme(legend.text = element_text(face = "bold.italic", size = 18 / .pt)) +

  coord_cartesian(ylim = c(0, 100)) +
  labs(x='Round 1 and Round 2 outcome', y='Lift probability in Round 3 (%)', color=NULL)

# write temporarily to file to render in qmd
ggsave('./presentation/replication-fig3c_behavioral_only.png', plot = replication.fig3c_behavioralonly, width=4, height=3)
```

:::: {.columns}

::: {.column .fragment}
![Figure 3C (Behavioral Only). Error bars indicate bootstrapped 95% CIs.](./fig3c_behavioral_only.png)

::: {.fragment}
Collaborators can succeed together even if they have failed alone (`r papaja::apa_print(original.ttest_result)$statistic`; $d = `r papaja::apa_num(original.cohensd$Cohens_d)`$)
:::
:::

::: {.column .fragment}
![Replication (Behavioral Only). Error bars indicate bootstrapped 95% CIs.](./replication-fig3c_behavioral_only.png)

::: {.fragment}
**Replicated!** (`r papaja::apa_print(replication.ttest_result)$statistic`; $d = `r papaja::apa_num(replication.cohensd$Cohens_d)`$)
:::
:::

::::

## Replication Results: Exploratory

:::: {.columns}

::: {.column .fragment}
![Figure 3C. Model simulations averaged across 10 iterations. Error bars indicate bootstrapped 95% CIs.](../writeup/original-fig3c.png)

Collaborators' chance of succeeding together gradually increased with more individual successes
:::

::: {.column .fragment}
![Replication. Model simulations consider strengths from 1 to 10 (inclusive) in steps of 0.03. Error bars indicate bootstrapped 95% CIs.](../writeup/replication-fig3c.png)

**Replicated!**
:::

::::

## Replication Results

- Replicated: joint success is possible even if there is no prior evidence of individual success, AND joint success likelihood increases as cases of individual success increases
  - In conjunction, these results **support the "joint effort" model of reasoning about collaborators' effort**

<!-- TODO:
massage the language (incl. editing speaker notes to be one line per keypress), and to try to make the conclusion clearer - could make the language here mirror the language in the introduction 
-->

## Discussion

- Replication success was expected due to large effect size and clear pattern
- I observed slight differences:
  - larger CIs
  - a flat section of the curve (i.e. similarity between two scenarios) not seen before; could be explained by the safe joint effort model (can show the figure that includes the safe joint effort model)
  - Effect size (_d_) was lower than original, as should be expected

::: {.notes}
Maybe the larger CIs are due to more variance than in the original?
:::

## Discussion

- To validate the "joint effort" model further, I will quantify the model fits (and confirm with new data), modeling within-participant variability independently from scenario condition

::: {.fragment}
![Figure 3C Replication with participant-level data.](../writeup/replication-fig3c-withrawdata.png){height=430}
:::

::: {.notes}
Within-participant variability can be seen as amplified, diminished, or biased probability judgments across scenarios that could be attributed to the participant rather than to the scenarios; we see that there could be a lot of this variability by looking at the heterogeneity in the underlying data

One way I might quantify the model fits with both within-participant variability and between-scenario variability is to estimate the likelihood of each participant's data under each model then summarizing across participants to estimate the likelihood of each model

Maybe the greater variance in the data (driving the larger CI values) is due to this heterogeneity that might not have been present in the original data
:::

## References

::: {#refs}
:::

<!-- ## (Extra) The basic framework: inferences

We infer a person's competence _and_ effort _simultaneously_ by internally modeling their mutual contingencies, and the influence of incentive:

1. Effort turns competence into success
  - e.g., if they succeed, they must have some competence and have put some effort in
  - **Success means combined competence and effort is at least X**
2. They don't put in more effort than necessary, given their competence
  - e.g., if they are competent enough to succeed, they allocate the minimum effort needed given their competence
  - **Success given their competence means at most Y effort**
3. Effort is motivated by _incentive_
  - e.g., effort is costly, so if they succeed with low incentive, they must really _desire_ the reward (i.e., reward has high utility to them)
  - **Success means at least Z desire for the reward and the minimum W effort that this motivates**

::: {.notes}
Also:
- (Practical assumption) Competence does not change across observations
- (Practical assumption) Competence and effort each have a defined range (0% to 100%)
::: -->
