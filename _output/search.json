[
  {
    "objectID": "presentation/index.html#collaboration-is-fundamental-to-humans",
    "href": "presentation/index.html#collaboration-is-fundamental-to-humans",
    "title": "Thinking About Collaborators",
    "section": "Collaboration is fundamental to humans",
    "text": "Collaboration is fundamental to humans\n\nFundamental to human success is how we’ve evolved to collaborate effectively (Tomasello et al., 2005)\nCollaboration requires judgments of what potential collaborators could bring to the table, and how much effort they will allocate\nHow complex are these judgments?\n\n\nCollaboration is fundamental to humans\nFundamental to human success is how we’ve evolved to collaborate effectively. There are many cognitive processes involved in this.\nFor example, in order to make decisions about who to work with, we make judgments of what potential collaborators could bring to the table, and how much effort they will allocate\nAs usual, evolution prefers the simplest solution to problems: so how complex is our solution?"
  },
  {
    "objectID": "presentation/index.html#how-do-we-judge-what-others-could-bring-to-the-table",
    "href": "presentation/index.html#how-do-we-judge-what-others-could-bring-to-the-table",
    "title": "Thinking About Collaborators",
    "section": "How do we judge what others could bring to the table?",
    "text": "How do we judge what others could bring to the table?\n\nWe observe them work, to update our beliefs about them (Bayesian inference) (Premack & Woodruff, 1978)\n\nBelief: how competent they are (Jara-Ettinger et al., 2015)\nBelief: how much effort they are putting in, given their competence and how much incentive there is (Jara-Ettinger et al., 2016)\n\n\n\nSo how do we judge what others could bring to the table?\nWell, we observe them working to update our beliefs about them; Bayesian inference is a good candidate for how we adjust our beliefs given observations of the world\nSome of our beliefs are about how competent they are\nand how much effort they are putting in\n\nWhile we also infer the difficulty of the task, Xiang et al. (2023) is seeking to integrate the independent role of perceived competence."
  },
  {
    "objectID": "presentation/index.html#how-do-we-judge-what-others-could-bring-to-the-table-and-how-much-effort-they-will-allocate",
    "href": "presentation/index.html#how-do-we-judge-what-others-could-bring-to-the-table-and-how-much-effort-they-will-allocate",
    "title": "Thinking About Collaborators",
    "section": "How do we judge what others could bring to the table, and how much effort they will allocate?",
    "text": "How do we judge what others could bring to the table, and how much effort they will allocate?\n\nSimple models of effort allocation: we could think that…\n\nSocial compensation: people assume others won’t apply any effort, so they allocate effort as though they were working alone (Williams & Karau, 1991) (solitary model)\nSocial loafing: people assume others will apply maximum effort, so they allocate only enough effort to help them (Karau & Williams, 1993; Latané et al., 1979) (compensatory model)\n\nXiang et al. (2023) propose a more complex model: we think that…\n\nJointly optimize effort: people infer how much effort others will allocate and how much the others think they will allocate, and then calibrate their effort accordingly – seeking to optimize their combined effort (joint effort model)\n\n\n\nAfter we infer each other’s competence and effort from observing their behavior, how do we judge how they much effort they will allocate?\nThere are a couple simple solutions that evolution could have chosen for how we calibrate our own effort -&gt; &lt;read slide&gt;\n“Optimize combined effort” means calibrate the combined effort to be enough to achieve the task (and for effort also not to be unfairly allocated)\n\nWe make some assumptions to make the inferences in the proposed model tractable (Jara-Ettinger et al., 2016): 1. If they succeed, they must have at least some competence and have put some effort in 2. If they are competent enough to succeed, they will allocate the minimum effort needed given their competence 3. Effort is costly, so if they succeed with low incentive, they must really desire the reward (i.e., reward has high utility to them) 4. They discount unequal effort allocation (i.e., they care somewhat about fairness) 5. (Practical assumption) Competence does not change across observations 6. (Practical assumption) Competence and effort each have a defined range (0% to 100%)\nIllustrating the basic framework of inferences:\nWe infer a person’s competence and effort simultaneously by internally modeling their mutual contingencies, and the influence of incentive:\n\nEffort turns competence into success\n\n\ne.g., if they succeed, they must have some competence and have put some effort in\nSuccess means combined competence and effort is at least X\n\n\nThey don’t put in more effort than necessary, given their competence\n\n\ne.g., if they are competent enough to succeed, they allocate the minimum effort needed given their competence\nSuccess given their competence means at most Y effort\n\n\nEffort is motivated by incentive\n\n\ne.g., effort is costly, so if they succeed with low incentive, they must really desire the reward (i.e., reward has high utility to them)\nSuccess means at least Z desire for the reward and the minimum W effort that this motivates"
  },
  {
    "objectID": "presentation/index.html#xiang2023-zo-lift-that-box",
    "href": "presentation/index.html#xiang2023-zo-lift-that-box",
    "title": "Thinking About Collaborators",
    "section": "Xiang et al. (2023): Lift That Box!",
    "text": "Xiang et al. (2023): Lift That Box!\n\nXiang et al. (2023) used a game-like task to collect participants’ judgments of competence, effort, and likelihood of success of virtual collaborators, in order to see how well the joint effort model predicted these judgments \n\n\nThe authors then sought to validate their more complex model\n&lt;Read slide. Then demo the task and point out the rounds 1-2 are individual, and round 3 is joint activity&gt;\nSo they had many different results, but let’s take a look at the key ones I was interested in replicating"
  },
  {
    "objectID": "presentation/index.html#xiang2023-zo-results",
    "href": "presentation/index.html#xiang2023-zo-results",
    "title": "Thinking About Collaborators",
    "section": "Xiang et al. (2023): Results",
    "text": "Xiang et al. (2023): Results\n\nParticipants thought that collaborators can succeed together even if they had always failed alone (\\(t(49) = 10.42\\), \\(p &lt; .001\\); \\(d = 1.47\\))\nParticipants also thought that collaborators’ chance of succeeding together increased with more individual successes (i.e., collaborators inferred each other’s competence and effort from prior attempts)\nOnly the proposed joint effort model predicted both:\n\n\n\n\n\n(Preview) Notice F,F;F,F, the left-to-right trend, and that joint effort fits the best.\n\n\n\n\n&lt;Read slide&gt;\nThis evidence suggests that their model is worth the complexity!\n\nAlso, they found that the increase was gradual (i.e., collaborators tried to distribute effort fairly)"
  },
  {
    "objectID": "presentation/index.html#replicating-xiang2023-zo-power",
    "href": "presentation/index.html#replicating-xiang2023-zo-power",
    "title": "Thinking About Collaborators",
    "section": "Replicating Xiang et al. (2023): Power",
    "text": "Replicating Xiang et al. (2023): Power\n\nI estimated that we needed only 9 participants (as this would give us 97.05% power for the main effect)\nHowever, to replicate the qualitative pattern, in the absence of a good expectation of how many participants were necessary for this, I chose to collect the full 50 participants\n\nAs a side-effect, this will also enable future exploratory analysis of the dataset, which I’ll briefly describe later\n\n\n\n&lt;Read slide&gt;"
  },
  {
    "objectID": "presentation/index.html#replicating-xiang2023-zo-modeling",
    "href": "presentation/index.html#replicating-xiang2023-zo-modeling",
    "title": "Thinking About Collaborators",
    "section": "Replicating Xiang et al. (2023): Modeling",
    "text": "Replicating Xiang et al. (2023): Modeling\n\n\nFor the computational model, I chose to reimplement the model in memo, a new probabilistic programming language (Chandra et al., 2025) for performance and extensibility advantages (for future work)\n\nNeeded to validate that new model had comparable results\nWorked through distinguishing trivial implementation differences from implementation mistakes\nAccomplished this by changing one implementation variable at a time and measuring its impact to determine where discrepancies came from\n\n\n\nI expected the basic effect to replicate since there’s high signal to noise in the measure (high t-value and low SE),\nand I was 4/5 confident the qualitative pattern would replicate.\n&lt;Read slide&gt;\n\nWorked through distinguishing trivial implementation differences (e.g., going from a continuous prior to a discrete prior) from meaningful implementation discrepancies (e.g., errors in the model implementation)\nHow I changed one variable at a time in modeling: webppl MCMC -&gt; webppl enumerate -&gt; memo enumerate"
  },
  {
    "objectID": "presentation/index.html#replication-results-confirmatory",
    "href": "presentation/index.html#replication-results-confirmatory",
    "title": "Thinking About Collaborators",
    "section": "Replication Results: Confirmatory",
    "text": "Replication Results: Confirmatory\n\n\n\n\n\nFigure 3C (Behavioral Only). Error bars show bootstrapped 95% CIs.\n\n\n\nCollaborators can succeed together even if they have failed alone (\\(t(49) = 10.42\\), \\(p &lt; .001\\); \\(d = 1.47\\))\n\n\n\n\n\nReplication (Behavioral Only). Error bars show bootstrapped 95% CIs.\n\n\n\nReplicated! (\\(t(49) = 9.26\\), \\(p &lt; .001\\); \\(d = 1.31\\))\n\n\n\n&lt;Read slide, explaining plots&gt;"
  },
  {
    "objectID": "presentation/index.html#replication-results-exploratory",
    "href": "presentation/index.html#replication-results-exploratory",
    "title": "Thinking About Collaborators",
    "section": "Replication Results: Exploratory",
    "text": "Replication Results: Exploratory\n\n\n\n\n\nFigure 3C. Model simulations averaged across 10 iterations. Error bars show bootstrapped 95% CIs.\n\n\nCollaborators’ chance of succeeding together increased with more individual success, validating joint effort model\n\n\n\n\nReplication. Model simulations use strengths in steps of .03. Error bars show bootstrapped 95% CIs.\n\n\nReplicated!\n\n\n&lt;Read slide, explaining plots&gt;\nIn sum, they replcicated that joint success is possible even if there is no prior evidence of individual success, AND joint success likelihood increases as cases of individual success increases. In conjunction, these results support the joint effort model of reasoning about collaborators’ effort"
  },
  {
    "objectID": "presentation/index.html#discussion",
    "href": "presentation/index.html#discussion",
    "title": "Thinking About Collaborators",
    "section": "Discussion",
    "text": "Discussion\n\nReplication success was expected due to large effect size and clear pattern\nI observed slight differences:\n\nLarger CIs\nFlat section of the curve (i.e., similarity between two scenarios) not seen before\nEffect size (d) was lower than original, as should be expected\n\n\n\n&lt;Read slide&gt;\n\nFlat part could be explained by the safe joint effort model (can show the figure that includes the safe joint effort model)"
  },
  {
    "objectID": "presentation/index.html#discussion-next-steps",
    "href": "presentation/index.html#discussion-next-steps",
    "title": "Thinking About Collaborators",
    "section": "Discussion: Next Steps",
    "text": "Discussion: Next Steps\n\nTo validate the joint effort model further, I will quantify the model fits (and confirm with new data), modeling within-participant variability independently from variability due to scenario (outcomes of rounds 1 and 2)\n\n\n\n\n\nFigure 3C Replication with participant-level data.\n\n\n\n\nNow some next steps…\n&lt;Read slide&gt;\nWe can see that there may be variance or even heterogeneity across participants that could be modeled; I’d like to estimate model fits, explicitly modeling for the between-participants variability, using model-likelihood measures for each participant then aggregating across participants.\nAlso, Maybe the greater variance in the data (driving the larger CIs) is due to this heterogeneity that might not have been present in the original data. Or it could just be due to publication bias/effect inflation.\n\nParticipant-level variability can be seen as amplified, diminished, or biased probability judgments across scenarios that could be attributed to the participant rather than to the scenarios; we see that there could be a lot of this variability by looking at the underlying participant-level data.\nOne way I might quantify the model fits with both types of variability is to estimate the likelihood of each participant’s data under each model then summarizing across participants to estimate the likelihood of each model."
  },
  {
    "objectID": "presentation/index.html#discussion-task",
    "href": "presentation/index.html#discussion-task",
    "title": "Thinking About Collaborators",
    "section": "Discussion: Task",
    "text": "Discussion: Task\n\nSome limitations:\n\nThe task is not very naturalistic (limits ecological validity)\nThe participant isn’t a collaborator themselves (limits construct validity)\n\nFuture steps:\n\nRich textual vignettes that paint a fuller scene than just stick figures before and after they attempt to lift boxes\nInclude the participant as a collaborator, e.g., with their own incentive related to the virtual collaborators’ outcome\n\n\n\nLast,\nI want to acknowledge describe some limitations of the task -&gt; &lt;read slide&gt;"
  },
  {
    "objectID": "presentation/index.html#thank-you",
    "href": "presentation/index.html#thank-you",
    "title": "Thinking About Collaborators",
    "section": "Thank you!",
    "text": "Thank you!\n\nQuestions?"
  },
  {
    "objectID": "presentation/index.html#references",
    "href": "presentation/index.html#references",
    "title": "Thinking About Collaborators",
    "section": "References",
    "text": "References\n\n\nChandra, K., Chen, T., Tenenbaum, J. B., & Ragan-Kelley, J. (2025). A domain-specific probabilistic programming language for reasoning about reasoning (or: A memo on memo). Proc. ACM Program. Lang., 9(OOPSLA2). https://doi.org/10.1145/3763078\n\n\nJara-Ettinger, J., Gweon, H., Schulz, L. E., & Tenenbaum, J. B. (2016). The na&#xef;ve utility calculus: Computational principles underlying commonsense psychology. Trends in Cognitive Sciences, 20(8), 589–604. https://doi.org/10.1016/j.tics.2016.05.011\n\n\nJara-Ettinger, J., Gweon, H., Tenenbaum, J. B., & Schulz, L. E. (2015). Children’s understanding of the costs and rewards underlying rational action. Cognition, 140, 14–23. https://doi.org/https://doi.org/10.1016/j.cognition.2015.03.006\n\n\nKarau, S. J., & Williams, K. D. (1993). Social loafing: A meta-analytic review and theoretical integration. Journal of Personality and Social Psychology, 65(4), 681–706. https://doi.org/10.1037/0022-3514.65.4.681\n\n\nLatané, B., Williams, K., & Harkins, S. (1979). Many hands make light the work: The causes and consequences of social loafing. Journal of Personality and Social Psychology, 37(6), 822–832. https://doi.org/10.1037/0022-3514.37.6.822\n\n\nPremack, D., & Woodruff, G. (1978). Does the chimpanzee have a theory of mind? Behavioral and Brain Sciences, 1(4), 515–526. https://doi.org/10.1017/S0140525X00076512\n\n\nTomasello, M., Carpenter, M., Call, J., Behne, T., & Moll, H. (2005). Understanding and sharing intentions: The origins of cultural cognition. Behavioral and Brain Sciences, 28(5), 675–691. https://doi.org/10.1017/S0140525X05000129\n\n\nWilliams, K. D., & Karau, S. J. (1991). Social loafing and social compensation: The effects of expectations of co-worker performance. J Pers Soc Psychol, 61(4), 570–581.\n\n\nXiang, Y., Vélez, N., & Gershman, S. J. (2023). Collaborative decision making is grounded in representations of other people’s competence and effort. J. Exp. Psychol. Gen., 152(6), 1565–1579."
  },
  {
    "objectID": "writeup/index.html",
    "href": "writeup/index.html",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "",
    "text": "How do individuals perceive, represent, and judge potential collaborators’ affordances for collaboration? Xiang et al. (2023) propose and empirically validate a probabilistic model in which jointly inferred estimates of competence and effort, given observed outcomes (“belief–desire–competence framework”), predict individuals’ judgments of collaborators, across various cognitive tasks which are common within collaboration. In particular, in Experiment 1, their model best predicts participants’ judgments about whether joint activity will succeed, compared to plausible alternative models. I sought to replicate this finding to help establish the robustness of this model of judgments about collaborators, before I might extend it in future work.\nExperiment 1 involves observing contestants try to lift a heavy box in a set of contests (six total), each containing three rounds. In each contest, there are two unique avatars playing as the contestants. In rounds 1 and 2, each contestant attempts to lift the box by themselves. In round 3, the contestants attempt to lift the box together. In each round, after observing each contestant succeed or fail to lift the box, the participant judges the strength of each contestant (1–10), and for each contestant that successfully lifted the box, the participant judges that contestant’s allocated effort (0%–100%). At the start of round 2, the participant judges each contestant’s probability of lifting the box successfully (0%–100%), prior to observing the lift attempts. At the start of round 3, the participant judges the probability that the contestants will successfully lift the box together. Further, an incentive to the contestants for lifting the box is specified in each round: in the second and third rounds, the specified reward for lifting the box is double that of the first round. Participants are also told that the heavy box had a constant weight across all contests, which effectively requires at least 5 strength points applied (e.g., on average, 100% effort for a contestant with strength 5, or 50% effort for a contestant with strength 10, etc.). Last, participants see a progressively filled-out table of all the lift outcomes during each contest, visible throughout, including when making judgments.\n\n\n\nExample illustrations shown during instructions; note that the incentive displayed for round 1 is $10, not $20.\n\n\n\n\n\nAn example table for a contest, as it would appear during the probability judgment at the beginning of round 3.\n\n\nI sought to replicate that A) when rounds 1 and 2 feature no individual success, participants still judge the probability of joint success in round 3 as non-zero. In addition to this confirmatory analysis, I sought to qualitatively replicate that B) participants judge higher probability of joint success as cases of individual success increases, and that the proposed model is the only model evaluated which predicts both patterns A and B. See Analysis Plan below for more details.\n\n\n\n\n\n\n\nNoteLinks\n\n\n\nRepository: https://github.com/psyc-201/xiang2023\nOSF Component with Preregistration: https://osf.io/89ae4/\nOriginal paper: 2023_xiang_effort.pdf (Retrieved on Sep 30, 2025 from velezlab.org)\nHosted experiment: https://psyc-201.github.io/xiang2023/exp/index1.html\nHosted version of this report: https://psyc-201.github.io/xiang2023/writeup/\nPresentation accompanying this report: https://psyc-201.github.io/xiang2023/presentation/"
  },
  {
    "objectID": "writeup/index.html#introduction",
    "href": "writeup/index.html#introduction",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "",
    "text": "How do individuals perceive, represent, and judge potential collaborators’ affordances for collaboration? Xiang et al. (2023) propose and empirically validate a probabilistic model in which jointly inferred estimates of competence and effort, given observed outcomes (“belief–desire–competence framework”), predict individuals’ judgments of collaborators, across various cognitive tasks which are common within collaboration. In particular, in Experiment 1, their model best predicts participants’ judgments about whether joint activity will succeed, compared to plausible alternative models. I sought to replicate this finding to help establish the robustness of this model of judgments about collaborators, before I might extend it in future work.\nExperiment 1 involves observing contestants try to lift a heavy box in a set of contests (six total), each containing three rounds. In each contest, there are two unique avatars playing as the contestants. In rounds 1 and 2, each contestant attempts to lift the box by themselves. In round 3, the contestants attempt to lift the box together. In each round, after observing each contestant succeed or fail to lift the box, the participant judges the strength of each contestant (1–10), and for each contestant that successfully lifted the box, the participant judges that contestant’s allocated effort (0%–100%). At the start of round 2, the participant judges each contestant’s probability of lifting the box successfully (0%–100%), prior to observing the lift attempts. At the start of round 3, the participant judges the probability that the contestants will successfully lift the box together. Further, an incentive to the contestants for lifting the box is specified in each round: in the second and third rounds, the specified reward for lifting the box is double that of the first round. Participants are also told that the heavy box had a constant weight across all contests, which effectively requires at least 5 strength points applied (e.g., on average, 100% effort for a contestant with strength 5, or 50% effort for a contestant with strength 10, etc.). Last, participants see a progressively filled-out table of all the lift outcomes during each contest, visible throughout, including when making judgments.\n\n\n\nExample illustrations shown during instructions; note that the incentive displayed for round 1 is $10, not $20.\n\n\n\n\n\nAn example table for a contest, as it would appear during the probability judgment at the beginning of round 3.\n\n\nI sought to replicate that A) when rounds 1 and 2 feature no individual success, participants still judge the probability of joint success in round 3 as non-zero. In addition to this confirmatory analysis, I sought to qualitatively replicate that B) participants judge higher probability of joint success as cases of individual success increases, and that the proposed model is the only model evaluated which predicts both patterns A and B. See Analysis Plan below for more details.\n\n\n\n\n\n\n\nNoteLinks\n\n\n\nRepository: https://github.com/psyc-201/xiang2023\nOSF Component with Preregistration: https://osf.io/89ae4/\nOriginal paper: 2023_xiang_effort.pdf (Retrieved on Sep 30, 2025 from velezlab.org)\nHosted experiment: https://psyc-201.github.io/xiang2023/exp/index1.html\nHosted version of this report: https://psyc-201.github.io/xiang2023/writeup/\nPresentation accompanying this report: https://psyc-201.github.io/xiang2023/presentation/"
  },
  {
    "objectID": "writeup/index.html#methods",
    "href": "writeup/index.html#methods",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "Methods",
    "text": "Methods\n\nPower & Precision Analysis\n\n\nCode\n# adapted from ./original_code/competence_effort/Code/main_text/regression.R\n\nsuppressPackageStartupMessages(library(tidyverse))\n\noriginal.dat &lt;- local({\n  dat &lt;- read.csv('./original_code/competence_effort/Data/exp1.csv', header = T, stringsAsFactors = T)\n  dat$agent &lt;- factor(dat$agent, labels = c('A', 'B'))\n  dat$round &lt;- as.factor(dat$round)\n  dat$scenario &lt;- factor(dat$scenario, levels = c('F,F;F,F','F,F;F,L','F,L;F,L','F,F;L,L','F,L;L,L','L,L;L,L'), ordered = T)\n  dat$prob[dat$round==3 & dat$agent=='B'] &lt;- NaN # so that the probabilities are only counted once for joint lifting\n  dat &lt;- arrange(dat,subject,scenario,round,agent)\n  dat$subject &lt;- as.factor(dat$subject)\n  dat$model &lt;- 'data'\n  dat\n})\n\n# Test lift probability\noriginal.ttest_result &lt;- local({\n  t.test(\n    # filter one agent so that the probability is only counted once. Round 3 is a joint lift.\n    original.dat$prob[original.dat$round == 3 & original.dat$scenario == 'F,F;F,F' & original.dat$agent == 'A'],\n    mu = 0,\n    alternative = 'two.sided'\n  )\n})\n\noriginal.cohensd &lt;- effectsize::effectsize(original.ttest_result)\n\n\nThe original effect size is \\(d = 1.47\\).  To achieve 80%, 90%, or 95% power to detect that effect size, I needed:\n\n\n\nTarget Power\nNecessary N\nEstimated Power\n\n\n\n\n80%\n6\n81.9866034%\n\n\n90%\n8\n94.4324728%\n\n\n95%\n9\n97.0458054%\n\n\n\nCollecting 9 participants was completely feasible (with each spending ~16 minutes; see Procedure). To compensate for the original study’s measured effect size being inflated (e.g., due to publication bias), 15 participants would be necessary for sufficient power for the primary effect of interest, and this count was also feasible.\nHowever, for the sake of qualitatively replicating the overall pattern of estimated probabilities increasing as scenarios feature more successful individual lifts (general increase in probability from left to right in Figure 2; which I’ve labeled “B” above), and that the joint effort model is the only considered model which captures both the key effect and this pattern, I found it prudent to retain the same sample size so that I might attempt to replicate the original qualitative finding with comparable precision. (Targeting higher precision could potentially counter aforementioned effect inflation present in the original paper, but as it is unclear how to quantify the necessary precision for these qualitative analyses, targeting the original precision appears to be a sensible option.)\nFurther, in the original study, given 50 participants, the key effect appeared quite robust, with the CI for the data from scenario “F,F;F,F” being quite far from the line LiftProbability=0 (see Figure 2), suggesting that with 50 participants, I was nearly guaranteed to replicate this effect if it is true.\nGiven my understanding of course funding available, collecting 50 participants was feasible.\n\n\nPlanned Sample\nBased on my power and precision considerations, I planned to collect 50 participants on Prolific (from the U.S., per the original authors’ private note), following the specific exclusion strategy reported below.\nI followed precisely the data collection strategy in the original Experiment 1, as described on p. 1569:\n\n“Participants’ demographic information was not collected. Participants completed a comprehension check before they moved on to the experiment. They were not allowed to proceed until they answered all the comprehension check questions correctly. … To ensure data quality, we included two attention-check questions in the experiment. Participants who failed one attention check were warned immediately to pay closer attention. Participants who failed both attention checks were asked to leave the experiment and they were not counted among the 50 participants we recruited. A total of 10 participants failed one attention check, and we did not exclude their data in our analysis.”\n\nIn the original study, per the experiment code, participants who fail both attention checks are asked to return their submission (to self-exclude). Per our current IRB approval, we did not ask these participants to do so and would instead manually exclude them from our target sample and analysis.\n\n\nMaterials\nI followed precisely the original Experiment 1. The materials for the experiment are available at https://github.com/jczimm/competence_effort. This repository is a clone of the authors’ own code repository for the paper, with the experimental task code copied from the authors’ hosted version linked in their README.md file.\nSee Procedure below for more details and a figure from the paper visualizing the task.\n\n\nProcedure\nI would follow precisely the original Experiment 1, as described on pp. 1569-1570:\n\n“… participants provided informed consent prior to the experiment. … Participants observed six contests between different pairs of contestants (see Table 2 for a description of the contests; the order was randomized). In each contest, the contestants were given three attempts to lift a box, corresponding to three rounds. In the first two rounds, the contestants tried lifting the box themselves. The reward for lifting the box was $10 in Round 1 and $20 in Round 2. In the third round of each contest, the two contestants tried to lift thebox together for a reward of $20 each. Participants first saw the lift outcome of Round 1 and made strength judgments (1–10; 1 means extremely weak and 10 means extremely strong) and effort judgments (0%–100%) for each contestant. For Rounds 2 and 3, they predicted the probability of the contestants lifting the box (0%–100%) before seeing the outcome, then observed the actual outcome and made strength and effort judgments. Note that participants made effort judgments only when the outcome was Lift. Participants were informed that the weight of the box was always the same and equivalent to a strength of 5 (i.e., an average contestant with strength 5 exerting all of their efforts would be able to lift the box). Participants also saw a table of all the previous outcomes when making their guesses. Figure 2 shows an illustration of the task.”  \n\nTo summarise: there are six contests, each with three rounds, and multiple judgments during each round.\nAccording to the consent form in the original experiment, the study is estimated to take 15 minutes. Accordingly, adhering to Prolific’s recommended rate of compensation ($12/hr), base compensation will be $3.\nFor determining bonus compensation, I followed precisely the original scheme as described on p. 1569:\n\n“Participants received … a potential bonus payment of up to $1. The amount of bonus they received was equal to the probability they put on the realized lift outcome on a randomly picked round.”\n\n\n\nAnalysis Plan\nI conducted a one-sample \\(t\\) test on the participants’ reported round-3 lift probability for scenario “F,F;F,F” (i.e., each contestant failed to lift the box in both rounds 2 and 3), with the null hypothesis that the true lift probability is zero. See Planned Sample for the data exclusion rule.\nIn addition to this confirmatory analysis, I would attempt to replicate the qualitative pattern that participants judge higher probability of joint success as cases of individual success increases. I would also reimplement their proposed model (joint effort) and alternative models (solitary effort and compensatory effort) in memo (Chandra et al., 2025) (see Differences from Original Study) and attempt to replicate that the proposed model is the only model evaluated which predicts both the qualitative pattern and the other result (that participants estimate round-3 lift probability as non-zero after two rounds of failures). See original paper’s Figure 3C and accompanying explanation, pp. 1570-1571; I would recreate that figure and attempt to replicate that explanation on my own data. Further, I would also estimate and statistically test the difference in the probability judgments plotted in Figure 3C between the original data and my new data.\nI must note that while the \\(t\\) test alone does not justify the paper’s central claim that the joint effort model is qualitatively predictive, in the absence one such statistical test, this \\(t\\) test is a good alternative: this test demonstrates one of two key behavioral effects (p. 1570) which the model was qualitatively evaluated to predict. (In other words, I should attempt to replicate this behavioral effect before I could try to qualitatively replicate that the joint effort model is the only considered model which can predict it.)\n\n\nDifferences from Original Study\nThe sample differed in that the original was from Amazon Mechanical Turk, while the new sample is from Prolific. Accordingly, base compensation was be $3 instead of $2.\nThe only difference in setting is that the task was hosted at https://psyc-201.github.io/xiang2023/exp/index1.html rather than https://gershmanlab.com/experiments/yang/toc/Experiment/index1.html. Accordingly, I also replaced the original authors’ data-saving step (a php page) with DataPipe.\nThe only known visible differences in procedure are:\n\nThe consent form is updated according to the details provided by the UCSD course PSYC 201A\nIn task instructions, “HIT” is renamed “submission”\nIn task instructions, “Different pairs of contestants will come in to lift the box” becomes “For each contest, a new pair of contestants will come in to lift the box” (to clarify first two questions of comprehension check)\nSee Differences from pre-data collection methods plan for an additional change\n\nWhile the confirmatory analysis was rerun using the original R code provided by the authors, for the qualitative replication I used a reimplementation of the joint effort model and the alternative models (solitary effort and compensatory effort models) in memo (Chandra et al., 2025), based on the authors’ original implementations in WebPPL. I expected that reimplementation of the continuous probabilistic models in WebPPL as discrete probabilistic models in memo would produce higher estimate precision as the model predictions will be deterministic, rather than stochastic. (I also discretized the distribution at a high resolution such that there was no meaningful systematic difference in estimates.)\nI did not anticipate that these differences are meaningful regarding the original paper’s claims and the analyses of interest. However, it is possible that 3 and 4 - improving comprehension-check fairness and reducing testing fatigue - may slightly improve measurement precision and reduce measurement bias, enhancing the claims of my replication.\n\n\nReliability and Validity\nThe key measure is the participant’s report of the probability of the contestants successfully lifting the box in round 3. For this measure, the latent construct of interest is the participant’s internal estimate of lift probability.\nThe reliability of this measure is unclear, given a lack of referenced evidence of the reliability of an explicit probability judgment. However, in the current model of the participant’s judgment as Bayesian inference, I infer that the reliability is negatively correlated with the noisiness of the participant’s internal translation from implicit posterior distribution (over lift probability) to explicit report; in other words, this measure can be no more reliable than the reliability of participants’ own estimation of the expected value of their internal posterior distribution. As there may also be individual differences in the noisiness of this translation process reliability could be reduced by this.\nLikewise, validity is unclear. And following the same logic, I infer that the validity may vary across participants, given that there could be individual differences in the accuracy of participations’ estimation of the expected value of their internal posterior distribution.\n\n\nMethods Addendum (Post Data Collection)\n\nActual Sample\nWe collected data from 50 participants.\nOn Prolific there were 51 participants collected, not 50, since one returned the study but was also approved (since they returned the study, it opened up the slot again). However, we only have 50 participants’ data, since one participant’s data (prolificId starting with 5c6) wasn’t recorded; I think they didn’t press “Next” at the very end of the study, which is necessary to save the data. (In any future iterations, saving should happen before the completion code is given.)\nDemographic information was not collected. By design of the experiment, participants who failed the comprehension check twice were excluded (see below). Following the planned data exclusion rule, 0  participants were excluded for failing both attention checks. A total of 10  participants failed one attention check, and we did not exclude their data in our analysis.\n\n\nDifferences from pre-data collection methods plan\nAfter piloting, to align with Prolific’s guidelines, I modified the task such that no more than two attempts of the comprehension check were allowed. Participants who failed the comprehension check twice were asked to return the submission. Accordingly, before the comprehension check, I added: “You will have two chances to complete the comprehension questions correctly before we ask you to return your submission, per Prolific guidelines.”; and upon comprehension check failure, I added: “Please note that you have only X attempt(s) left.”\nUnless marked otherwise, all results used only the sample collected after this change was made."
  },
  {
    "objectID": "writeup/index.html#results",
    "href": "writeup/index.html#results",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "Results",
    "text": "Results\n\nData preparation\nThe data collected in this experiment included task-level data:\n\nsubjectId, encryptedProlificId\npre-set round outcomes (round1_o, round2_o, round3_o)\npre-set round incentives (round1_reward, round2_reward, round3_reward)\nnumber of failed attention checks (attention_sum)\n\nThey also included round-level data:\n\ncontest index (contest)\nround index (round)\nstrength (r1_strength_a/r2_strength_a/r3_strength_a, and r1_strength_b/r2_strength_b/r3_strength_b)\neffort (r1_effort_a/r2_effort_a/r3_effort_a, and r1_effort_b/r2_effort_b/r3_effort_b)\noutcomes (r1_outcome_a/r2_outcome_a/r3_outcome_a, and r1_outcome_b/r2_outcome_b/r3_outcome_b)\nprobability (r1_prob_a/r2_prob_a/r3_prob_a, and r1_prob_b/r2_prob_b/r3_prob_b)\n\nI would manually exclude any participants who failed both attention checks.\n\n\nCode\n### Data Preparation\n\n#### Load Relevant Libraries and Functions\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(lmerTest)\n\n# Helper\nDt &lt;- function(data, options=list(autoWidth=TRUE)) DT::datatable(data, extensions = c('Responsive'), rownames = FALSE, options = options) \n\n#### Import data\n\nfilename_to_subjectId &lt;- function(filename) str_extract(filename, '(?&lt;=index1_).*(?=_output(-anon)?\\\\.csv)')\n\nif (str_equal(Sys.getenv('OSF_PAT'), '')) {\n  warning(\"OSF_PAT not found in .Renviron; cannot download data from OSF. Continuing, assuming that anonymized data is already accessible\")\n} else {\n  message(\"Retrieving data on OSF...\")\n  # (data downloaded from OSF, to which is was saved by datapipe)\n  # OSF is authenticated automatically using OSF_PAT in .Renviron\n  OSF_NODE_REPLICATION_PROJECT &lt;- '89ae4'\n  osf_data_files &lt;- osfr::osf_retrieve_node(OSF_NODE_REPLICATION_PROJECT) %&gt;%\n    osfr::osf_ls_nodes(pattern = \"Prolific\") %&gt;%\n    osfr::osf_ls_files(n_max=Inf)\n\n  message(\"Downloading any new data...\")\n  osf_data_files %&gt;%\n    osfr::osf_download(path=\"./data\", conflicts=\"skip\", progress=interactive()) %&gt;%\n    invisible() # if sanity checks point to any data being missing (possibly due to the csvs being malformed), use conflicts=\"overwrite\"\n\n  message(\"Extracting metadata...\")\n  dat_filemeta &lt;- osf_data_files %&gt;%\n    rename(datapipe_meta = meta) %&gt;%\n    unnest_wider(datapipe_meta, names_sep=\".\") %&gt;%\n    unnest_wider(datapipe_meta.attributes, names_sep=\".\") %&gt;%\n    select(name, datapipe_meta.attributes.date_modified) %&gt;%\n    mutate(subjectId = filename_to_subjectId(name)) %&gt;%\n    select(-name) %&gt;%\n    distinct() # in case there's any duplicates for any reason\n\n  message(\"Saving anonymous data...\")\n  warning(\"**Assuming data contents are already anonymous! (i.e., all prolificIDs are still encrypted and the written responses contain no identifying information). If they're not anonymous, you should anonymize them now.\")\n  local({\n    # Read the data now, add in the used metadata, and resave it with new filenames tagging them as anonymous\n    pattern_match_original_raw_data &lt;- 'index1_(.*)_output.csv'\n    original_filenames &lt;- list.files(path = './data', pattern = pattern_match_original_raw_data, full.names = T)\n    dat_raw_anonymized &lt;- original_filenames %&gt;%\n      map(~ {\n        read.csv(.x, header = T, stringsAsFactors = F) %&gt;%\n          mutate(subjectId = filename_to_subjectId(basename(.x))) %&gt;%\n          left_join(dat_filemeta, by=\"subjectId\")\n      })\n    \n    pattern_name_anonymized_raw_data &lt;- 'index1_\\\\1_output-anon.csv'\n    new_filenames &lt;- stringr::str_replace(original_filenames, pattern_match_original_raw_data, pattern_name_anonymized_raw_data)\n    walk2(dat_raw_anonymized, new_filenames, ~ {\n      write.csv(.x %&gt;% select(-subjectId), file = .y, row.names = F)\n    })\n  })\n  message(\"Saved anonymous data!\")\n}\n\n# read all files, save subjectId to a column, and bind rows\npattern_match_anonymized_raw_data &lt;- 'index1_.*_output-anon.csv'\n\ndat_raw &lt;- list.files(path = './data', pattern = pattern_match_anonymized_raw_data, full.names = T) %&gt;%\n  map_dfr(~ {\n    read.csv(.x, header = T, stringsAsFactors = F) %&gt;%\n      mutate(subjectId = filename_to_subjectId(basename(.x)))\n  })\n\ndat_raw_annotated &lt;- dat_raw %&gt;%\n  group_by(subjectId) %&gt;%\n  mutate(passed_an_attention_check = all(attention_sum &gt;= 1, na.rm=T)) %&gt;%\n  ungroup() %&gt;%\n  mutate(collection_phase = ifelse(datapipe_meta.attributes.date_modified &gt; lubridate::ymd(\"2025-11-20\", tz=\"PST\"), \"main\", \"pilot\"), collection_phase=ifelse(is.na(collection_phase), \"pilot\", collection_phase), collection_phase=factor(collection_phase, levels=c(\"pilot\", \"main\")))\n\n#### Data exclusion / filtering\n# based on attention checks criterion\n# (also only include participants who have a session ID, as those who don't were not real Prolific participants; and only include participants who have done a trial, otherwise they have failed the comprehension check the maximum number of times)\n# and discard pilot data\ndat_raw_filtered &lt;- dat_raw_annotated %&gt;%\n  filter(passed_an_attention_check) %&gt;%\n  group_by(subjectId) %&gt;%\n  filter(any(!is.na(sessionId))) %&gt;%\n  filter(any(trial_type == \"survey-html-form\")) %&gt;%\n  ungroup() %&gt;%\n\n  filter(collection_phase == \"main\")\n\n#### Prepare data for analysis - create columns etc.\n# Munge to generate the rows/columns I need for the analysis, which are per-round, per-agent data: \n# - prob\n# - round\n# - scenario\n# - agent\n# - subject\n# For reference (from original_code/competence_effort/Data/README.txt):\n# agent column: 1 refers to the weaker contestant, 2 refers to the stronger contestant (if applicable) based on individual lifting outcomes.\n# prob column: Participants' lift probability judgments.\n\ndat_clean &lt;- dat_raw_filtered %&gt;%\n  filter(trial_type == \"survey-html-form\") %&gt;%\n  select(subject=subjectId, contest, round, outcome_a, outcome_b, probability_a, probability_b, strength_a, strength_b, effort_a, effort_b) %&gt;%\n  arrange(subject, contest, round) %&gt;%\n  \n  # collapse across round Stage, like in the open data (exp1.csv)\n  group_by(subject, contest, round) %&gt;%\n  summarise(\n    across(everything(), ~{\n      ifelse(any(!is.na(.x)), .x[!is.na(.x)], NA)\n    })\n  ) %&gt;%\n\n  # prepare to generate scenario column, which uses letters to represent the outcomes\n  ungroup() %&gt;%\n  mutate(\n    outcome_a = ifelse(outcome_a == 1, \"L\", \"F\"),\n    outcome_b = ifelse(outcome_b == 1, \"L\", \"F\")\n  ) %&gt;%\n\n  # relabel agents where needed: for any contest, if the two agents don't have the same outcome in any round, the agent that lifted should be called agent b for the whole contest\n  group_by(subject, contest) %&gt;%\n  pivot_longer(cols = c(ends_with(\"_a\"), ends_with(\"_b\")),\n               names_to = c(\".value\", \"agent\"),\n               names_sep = \"_\") %&gt;%\n  group_by(subject, contest, round) %&gt;%\n  mutate(\n    needs_switch =\n      length(unique(outcome)) &gt; 1 & \n      ((outcome == \"L\" & agent == \"a\") | (outcome == \"F\" & agent == \"b\"))\n  ) %&gt;%\n  group_by(subject, contest) %&gt;%\n  mutate(needs_switch = any(needs_switch)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    agent = ifelse(needs_switch, ifelse(agent == \"a\", \"b\", \"a\"), agent)\n  ) %&gt;%\n  select(-needs_switch) %&gt;%\n  pivot_wider(values_from = c(outcome, probability, strength, effort), \n              names_from = agent, \n              names_sep = \"_\") %&gt;%\n\n  # now in wide format, with the agents labeled as intended, define scenario\n  group_by(subject, contest, round) %&gt;%\n  mutate(outcome = paste(outcome_a, outcome_b, sep=\",\"), .after=\"round\") %&gt;%\n  group_by(subject, contest) %&gt;%\n  mutate(scenario = paste(outcome[round == 1 | round == 2], collapse=\";\"), .after=\"round\") %&gt;%\n  select(-starts_with(\"outcome\")) %&gt;%\n\n  # get into format used in the open data (exp1.csv)\n  pivot_longer(cols = c(ends_with(\"_a\"), ends_with(\"_b\")),\n               names_to = c(\".value\", \"agent\"),\n               names_sep = \"_\") %&gt;%\n  rename(prob = probability) %&gt;%\n  mutate(agent = ifelse(agent == 'a', 1, 2))\n\n\n\n\nCode\n# Decrypt prolific IDs to determine bonuses\n\nif (interactive()) { # safeguard to help ensure this is not output by quarto\n  private_key_path &lt;- './new_code/competence_effort/Experiment/private.pem'\n  if (!file.exists(private_key_path)) {\n    warning(\"Private key file not found at specified path. Cannot decrypt Prolific IDs without it.\")\n  } else {\n    decrypt_rsa &lt;- function(col, private_key_path) {\n      private_key &lt;- openssl::read_key(private_key_path)\n      decrypt_one &lt;- function(enc) {\n        if (is.na(enc) || enc == \"\") return(NA_character_)\n        tryCatch({\n          raw_enc &lt;- base64enc::base64decode(enc)\n          raw_dec &lt;- openssl::rsa_decrypt(raw_enc, private_key)\n          rawToChar(raw_dec)\n        }, error = function(e) NA_character_)\n      }\n      vapply(col, decrypt_one, FUN.VALUE = character(1))\n    }\n\n    dat_raw_annotated %&gt;%\n      select(subjectId, encryptedProlificId, bonus, passed_an_attention_check, datapipe_meta.attributes.date_modified) %&gt;%\n      group_by(subjectId) %&gt;%\n      filter(!is.na(bonus)) %&gt;%\n      ungroup() %&gt;%\n      # restore any subjects in case we lost them due to the filtering\n      complete(subjectId = dat_raw_annotated$subjectId) %&gt;%\n      complete(encryptedProlificId = dat_raw_annotated$encryptedProlificId) %&gt;%\n      mutate(\n        prolificId = decrypt_rsa(\n          encryptedProlificId,\n          private_key_path=private_key_path\n        )\n      ) %&gt;%\n      # filter to show only post-pilot participants\n      filter(collection_phase == \"main\") %&gt;%  \n      arrange(desc(prolificId)) %&gt;%\n      select(prolificId, bonus)\n      \n      # filter(!is.na(prolificId), bonus &gt; 0) %&gt;% format_csv(col_names=F)\n    \n    # NOTE: only do one bulk-bonus per batch of data collection, since might accumulate in Prolific if bulk-bonus multiple times!\n  }\n}\n\n\n\n\n\n\n\n\nNoteSanity Checks\n\n\n\n\n\n\n\nCode\n# Sanity checks\n\nmessage(\"count all participants with a sessionId\")\n\n\ncount all participants with a sessionId\n\n\nCode\n(real_participants &lt;- dat_raw_annotated %&gt;%\n  group_by(subjectId) %&gt;% filter(any(!is.na(sessionId))) %&gt;%\n  ungroup() %&gt;% distinct(subjectId) %&gt;%\n  nrow())\n\n\n[1] 97\n\n\nCode\nmessage(\"count participants included\")\n\n\ncount participants included\n\n\nCode\n(filtered_participants &lt;- dat_raw_filtered %&gt;%\n  ungroup() %&gt;% distinct(subjectId) %&gt;%\n  nrow())\n\n\n[1] 50\n\n\nCode\n(clean_participants &lt;- dat_clean %&gt;%\n  ungroup() %&gt;% distinct(subject) %&gt;%\n  nrow())\n\n\n[1] 50\n\n\nCode\nstopifnot(clean_participants == filtered_participants)\n\nmessage(\"count participants excluded due to failing both attention checks\")\n\n\ncount participants excluded due to failing both attention checks\n\n\nCode\n(excluded_for_2inattention &lt;- dat_raw_annotated %&gt;%\n  filter(!passed_an_attention_check) %&gt;%\n  group_by(subjectId) %&gt;%\n  filter(any(!is.na(sessionId))) %&gt;%\n  filter(any(trial_type == \"survey-html-form\")) %&gt;%\n  ungroup() %&gt;% distinct(subjectId) %&gt;%\n  nrow())\n\n\n[1] 0\n\n\nCode\nmessage(\"count participants kept because only failed one attention check\")\n\n\ncount participants kept because only failed one attention check\n\n\nCode\n(kept_despite_1inattention &lt;- dat_raw_filtered %&gt;%\n  group_by(subjectId) %&gt;%\n  filter(all(attention_sum &lt; 2, na.rm=T)) %&gt;%\n  ungroup() %&gt;% distinct(subjectId) %&gt;%\n  nrow())\n\n\n[1] 10\n\n\nCode\nmessage(\"check that contest vs scenario is uniformly distributed, and that each participant did each contest and each scenario once\")\n\n\ncheck that contest vs scenario is uniformly distributed, and that each participant did each contest and each scenario once\n\n\nCode\nset.seed(1)\ndat_clean %&gt;%\n  filter(agent == 1, round == 3) %&gt;%\n  ggplot(aes(x = scenario, y = contest)) +\n  geom_jitter(aes(color = subject), width=.1, height=.1) +\n  theme_minimal() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\nmessage(\"clearer way to check that each subject did each scenario once\")\n\n\nclearer way to check that each subject did each scenario once\n\n\nCode\ndat_clean %&gt;%\n  filter(agent == 1, round == 3) %&gt;%\n  ggplot(aes(x = scenario)) +\n  geom_col(stat=\"count\") +\n  theme_minimal() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\nmessage(\"clearer way to check that each subject did each contest once\")\n\n\nclearer way to check that each subject did each contest once\n\n\nCode\ndat_clean %&gt;%\n  filter(agent == 1, round == 3) %&gt;%\n  mutate(contest = factor(contest)) %&gt;%\n  ggplot(aes(x = contest)) +\n  geom_col(stat=\"count\") +\n  theme_minimal() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\nmessage(\"check that effort and strength plots have missing data in the right places\")\n\n\ncheck that effort and strength plots have missing data in the right places\n\n\nCode\ndat_clean %&gt;%\n  mutate(round = factor(round), agent = factor(agent, labels=c(\"Agent A\", \"Agent B\"))) %&gt;%\n  ggplot(aes(x = round, y=effort, color=subject, group=1)) + # set group since needed for geom_smooth to connect across the factor-type x=round\n  geom_point() + geom_line(aes(group=subject)) +\n  geom_smooth(method=lm, se=F, color=\"black\") +\n  facet_wrap(scenario~agent) +\n  theme_minimal() + theme(legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 800 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 800 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 500 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nCode\ndat_clean %&gt;%\n  mutate(round = factor(round), agent = factor(agent, labels=c(\"Agent A\", \"Agent B\"))) %&gt;%\n  ggplot(aes(x = round, y=strength, color=subject, group=1)) +\n  geom_point() + geom_line(aes(group=subject)) +\n  geom_smooth(method=lm, se=F, color=\"black\") +\n  facet_wrap(scenario~agent) +\n  theme_minimal() + theme(legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\ndat_clean %&gt;%\n  mutate(round = factor(round), agent = factor(agent, labels=c(\"Agent A\", \"Agent B\"))) %&gt;%\n  ggplot(aes(x = round, y=prob, color=subject, group=1)) +\n  geom_point() + geom_line(aes(group=subject)) +\n  geom_smooth(method=lm, se=F, color=\"black\") +\n  facet_wrap(scenario~agent) +\n  theme_minimal() + theme(legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 600 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 600 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 50 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\nCode\nmessage(\"review written responses\")\n\n\nreview written responses\n\n\nCode\ndat_raw_annotated |&gt;\n  filter(trial_type == \"survey-text\") |&gt;\n  select(subjectId, internal_node_id, responses, collection_phase) |&gt;\n  rowwise(subjectId) |&gt;\n  mutate(responses = jsonlite::fromJSON(responses)$Q0) |&gt;\n  group_by(subjectId) |&gt;\n  mutate(question = ifelse(internal_node_id == min(internal_node_id), \"DescribeTask\", \"Feedback\")) |&gt;\n  select(-internal_node_id) |&gt;\n  pivot_wider(names_from = \"question\", values_from = \"responses\") |&gt;\n  Dt()\n\n\n\n\n\n\nCode\nmessage(\"review comprehension checks\")\n\n\nreview comprehension checks\n\n\nCode\ndat_raw_annotated |&gt;\n  filter(trial_type == \"survey-multi-choice\") |&gt;\n  group_by(collection_phase, subjectId) |&gt;\n  summarise(checks = n()) |&gt;\n  group_by(collection_phase, checks) |&gt;\n  count()\n\n\n`summarise()` has grouped output by 'collection_phase'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 7 × 3\n# Groups:   collection_phase, checks [7]\n  collection_phase checks     n\n  &lt;fct&gt;             &lt;int&gt; &lt;int&gt;\n1 pilot                 1     8\n2 pilot                 2    10\n3 pilot                 3     2\n4 pilot                 5     1\n5 pilot                 9     2\n6 main                  1    23\n7 main                  2    58\n\n\nCode\ndat_raw_annotated |&gt;\n  filter(trial_type == \"survey-multi-choice\") |&gt;\n  group_by(subjectId) |&gt; mutate(attempts = n()) |&gt;\n  rowwise() |&gt; mutate(responses = list(jsonlite::fromJSON(responses))) |&gt;\n  select(subjectId, responses, attempts, collection_phase) |&gt; unnest_longer(responses) |&gt;\n  group_by(subjectId, responses, attempts) |&gt; mutate(times_answered = n()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x=responses, group=paste(times_answered, subjectId), fill=as.factor(times_answered))) +\n  geom_bar() +\n  geom_line(aes(y=after_stat(count)), stat=\"count\", position=position_stack(vjust=.5), alpha=.5) +\n  facet_grid(collection_phase~responses_id, scales = \"free\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = -20, hjust=0)) +\n  scale_fill_viridis_d() +\n  guides(color = \"none\") +\n  labs(\n    title = \"Comprehension check question responses (lines connect different attempts by same participant)\",\n    x = \"Response\",\n    y = \"Count\",\n    fill = \"Times answered\"\n  )\n\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfirmatory analysis\n\n\nCode\n# (Adapted from original_code/competence_effort/Code/main_text/regression.R)\n## Experiment 1\nreplication.dat &lt;- local({\n  dat &lt;- dat_clean # read.csv('./data/exp1.csv', header = T, stringsAsFactors = T)\n  dat$agent &lt;- factor(dat$agent, labels = c('A', 'B'))\n  dat$round &lt;- as.factor(dat$round)\n  dat$scenario &lt;- factor(dat$scenario, levels = c('F,F;F,F','F,F;F,L','F,L;F,L','F,F;L,L','F,L;L,L','L,L;L,L'), ordered = T)\n  dat$prob[dat$round==3 & dat$agent=='B'] &lt;- NaN # so that the probabilities are only counted once for joint lifting\n  dat &lt;- arrange(dat,subject,scenario,round,agent)\n  dat$model &lt;- 'data'\n  dat\n})\n\n# Lift probability\nreplication.ttest_result &lt;- local({\n  t.test(\n    # filter one agent so that the probability is only counted once. Round 3 is a joint lift.\n    replication.dat$prob[replication.dat$round == 3 & replication.dat$scenario == 'F,F;F,F' & replication.dat$agent == 'A'],\n    mu = 0,\n    alternative = 'two.sided'\n  )\n})\n\nreplication.cohensd &lt;- effectsize::effectsize(replication.ttest_result)\n\n\nI found that A) participants rated the probability of joint success in round 3 as non-zero (\\(M = 43.22\\), 95% CI \\([33.84, 52.60]\\), \\(t(49) = 9.26\\), \\(p &lt; .001\\)). See the black point range (data) for scenario “F,F;F,F” under Exploratory analyses.\nThe effect size found is \\(d = 1.31\\).\n\nCode\n# dat$prob[dat$round == 3 & dat$scenario == 'F,F;F,F' & dat$agent == 'A']\n\nalpha &lt;- .05\ncritical_tvalue &lt;- qt(1 - (alpha/2), df=replication.ttest_result$parameter) # dividing alpha by 2 since this is a two-tailed test\n\ndat &lt;- replication.dat\ndat %&gt;%\n  filter(round == 3, scenario == 'F,F;F,F', agent == 'A') %&gt;%\n  {ggplot(., aes(x=prob)) +\n    geom_histogram(bins=floor(sqrt(nrow(.)))) +\n    coord_cartesian(xlim = c(0, 100)) +\n    geom_vline(xintercept=critical_tvalue, color='red') +\n    theme_minimal() + labs(y='Density', x='Lift probability in Round 3 (%)')}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The red line shows the critical \\(t\\) value for detecting a statistically significant effect with \\(\\alpha = 0.05\\).\n\n\n\n\n\nExploratory analyses\nI found that B) the probability of joint success increased as individual success increased. Further, the joint effort model was the only model which predicted both pattern A (that joint success has non-zero probability; see Confirmatory analysis) and pattern B (joint success probability increases as individual success increases).\n\n\nCode\n# adapted from original_code/competence_effort/Code/main_text/plt.R and original_code/competence_effort/Code/main_text/plot_figures.R\nsimulation &lt;- read_csv(\"new_code/memo-sandbox/webppl vs memo/xiang2023-exp1-round3-model_fits_results.csv\") |&gt;\n  select(model, scenario, P) |&gt;\n  mutate(prob = 100*P)\nsimulation$scenario &lt;- factor(simulation$scenario, levels = c('F,F;F,F','F,F;F,L','F,L;F,L','F,F;L,L','F,L;L,L','L,L;L,L'), ordered = T)\n\nset.seed(1)\nfig3c &lt;- dat %&gt;%\n  ggplot(aes(scenario, prob, group = model, color = model)) +\n\n  geom_line(data = simulation %&gt;% filter(model=='compensatory')) +\n  geom_line(data = simulation %&gt;% filter(model=='solitary')) +\n  geom_line(data = simulation %&gt;% filter(model=='joint')) +\n  geom_point(data = simulation %&gt;% filter(model=='compensatory'), size=.8) +\n  geom_point(data = simulation %&gt;% filter(model=='solitary'), size=.8) +\n  geom_point(data = simulation %&gt;% filter(model=='joint'), size=.8) +\n\n  stat_summary(data = dat %&gt;% filter(round == 3 & agent == 'A'), fun.data = 'mean_cl_boot', geom = 'errorbar', width = .1) +\n  stat_summary(data = dat %&gt;% filter(round == 3 & agent == 'A'), fun = 'mean', geom = 'point', size = 1.5) +\n\n  scale_color_manual(\n    name = NULL,\n    labels = c('Data','Joint effort model','Solitary effort model','Compensatory effort model'),\n    values = c('#000000','#e35d5e','#004385','#05b2dc'),\n    limits = c('data','joint','solitary','compensatory'),\n    guide = ggh4x::guide_stringlegend(position = \"top\")\n  ) +\n  theme_minimal() + \n  theme(legend.text = element_text(face = \"bold.italic\", size = 18 / .pt)) +\n\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(x='Round 1 and Round 2 outcome', y='Lift probability in Round 3 (%)', color=NULL)\n\n# write temporarily to file to render in qmd\nggsave('./writeup/replication-fig3c.png', plot = fig3c, width=4, height=3)\n\noriginal.fig3c &lt;- local({\n  # adapted from original_code/competence_effort/Code/main_text/plot_figures.R\n\n  inference &lt;- read.csv('./original_code/competence_effort/Code/main_text/exp1_simulation.csv', header = T, stringsAsFactors = T)\n  inference$round &lt;- as.factor(inference$round)\n  inference$model &lt;- factor(inference$model, levels = c('joint','solitary','compensatory','maximum'), ordered = T)\n  inference$scenario &lt;- factor(inference$scenario, levels = c('F,F;F,F','F,F;F,L','F,L;F,L','F,F;L,L','F,L;L,L','L,L;L,L'), ordered = T)\n  inference$effort[inference$outcome==0] &lt;- NaN\n  inference$prob[inference$round==3 & inference$agent=='B'] &lt;- NaN\n\n  inference_subset &lt;- inference %&gt;% filter(round == 3 & agent == 'A')\n\n  set.seed(1)\n  original.dat %&gt;%\n    ggplot(aes(scenario, prob, group = model, color = model)) +\n\n    geom_line(data = inference_subset %&gt;% filter(model=='compensatory')) +\n    geom_line(data = inference_subset %&gt;% filter(model=='solitary')) +\n    geom_line(data = inference_subset %&gt;% filter(model=='joint')) +\n    geom_point(data = inference_subset %&gt;% filter(model=='compensatory'), size=.8) +\n    geom_point(data = inference_subset %&gt;% filter(model=='solitary'), size=.8) +\n    geom_point(data = inference_subset %&gt;% filter(model=='joint'), size=.8) +\n\n    stat_summary(data = original.dat %&gt;% filter(round == 3 & agent == 'A'), fun.data = 'mean_cl_boot', geom = 'errorbar', width = .1) +\n    stat_summary(data = original.dat %&gt;% filter(round == 3 & agent == 'A'), fun = 'mean', geom = 'point', size = 1.5) +\n\n    scale_color_manual(\n      name = NULL,\n      labels = c('Data','Joint effort model','Solitary effort model','Compensatory effort model'),\n      values = c('#000000','#e35d5e','#004385','#05b2dc'),\n      limits = c('data','joint','solitary','compensatory'),\n      guide = ggh4x::guide_stringlegend(position = \"top\")\n    ) +\n    theme_minimal() + \n    theme(legend.text = element_text(face = \"bold.italic\", size = 18 / .pt)) +\n    \n\n    coord_cartesian(ylim = c(0, 100)) +\n    labs(x='Round 1 and Round 2 outcome', y='Lift probability in Round 3 (%)', color=NULL)\n})\n\n# write temporarily to file to render in qmd\nggsave('./writeup/original-fig3c.png', plot = original.fig3c, width=4, height=3)\n\nset.seed(1)\nfig3c_w_raw_data &lt;- fig3c +\n  geom_line(aes(group=subject), linetype=\"solid\", linewidth=.25, alpha=.2, data = dat %&gt;% filter(round == 3 & agent == 'A'), position=position_jitter(width=.05, seed=0)) +\n  geom_point(aes(group=subject), alpha=.2, data = dat %&gt;% filter(round == 3 & agent == 'A'), size=.8, position=position_jitter(width=.05, seed=0))\n  \n# write temporarily to file to render in qmd\nggsave('./writeup/replication-fig3c-withrawdata.png', plot = fig3c_w_raw_data, width=6, height=4.5)\n\nfig3c_w_safejoint &lt;- fig3c +\n  geom_line(data = simulation %&gt;% filter(model=='safe_joint_w_gini')) +\n  geom_point(data = simulation %&gt;% filter(model=='safe_joint_w_gini'), size=.8) +\n  scale_color_manual(\n    name = NULL,\n    labels = c('Data','Joint effort','Safe joint effort','Solitary effort','Compensatory effort'),\n    values = c('#000000','#e35d5e','#b60cc5','#004385','#05b2dc'),\n    limits = c('data','joint','safe_joint_w_gini','solitary','compensatory'),\n    guide = ggh4x::guide_stringlegend(position = \"top\")\n  )\n  \n# write temporarily to file to render in qmd\nggsave('./writeup/replication-fig3c-withsafejoint.png', plot = fig3c_w_safejoint, width=6, height=4.5)\n\n\n\n\n\n\n\n\n\n\nFigure 2: Original (Figure 3C). “Model simulations averaged over 10 runs. Error bars indicate bootstrapped 95% confidence intervals.” (p. 1571) Regenerated error bars using a fixed seed.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Replication (Figure 3C). Model simulations consider strengths from 1 to 10 (inclusive) in steps of 0.03 (determined by memory constraints). Error bars indicate bootstrapped 95% confidence intervals.\n\n\n\n\n\n\n\n\n\n\n\nNoteFigure 3C Replication with Raw Data\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Figure 3C Replication with Raw Data\n\n\n\n\n\n\n\n\n\n\n\n\nNoteFigure 3C Replication with Safe Joint Model\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Figure 3C Replication with Safe Joint Model"
  },
  {
    "objectID": "writeup/index.html#discussion",
    "href": "writeup/index.html#discussion",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "Discussion",
    "text": "Discussion\n\nSummary of Replication Attempt\nThe primary, confirmatory result fully replicated: A) when rounds 1 and 2 feature no individual success, participants still judge the probability of joint success in round 3 as non-zero. The exploratory (qualitative) result also replicated: B) participants judge higher probability of joint success as cases of individual success increases. Further, I replicated that the authors’ proposed model was the only model evaluated which predicts both patterns A and B, comparing it to plausible alternative models. In conjunction, my replication supports the robustness of this model of reasoning about collaborators.\n\n\nCommentary\nReplication success was expected due to the large effect size and clear qualitative pattern. However, the replication success of the confirmatory result nevertheless reinforces my confidence in the reliability of the task paradigm, and the replication of the qualitative pattern using a model reimplementation reinforces my confidence in both that pattern and the modeling approach.\nIn my replication, I observed slightly larger CIs, and an effect size that was lower than the original. However, these may be expected due to publication bias. Further, I observed a slope inversion in the data means (i.e., estimated probability of round 3 succcess is lower for F,L;F,L than for F,F;F,L) which was not present in the data from the original study. Speculatively, this might be explainable by the safe joint effort model (see Figure 5), a modified version of the authors’ proposed joint effort model available in their supplement. In this case, the replication provides some support for that modified model."
  },
  {
    "objectID": "writeup/index.html#references",
    "href": "writeup/index.html#references",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "References",
    "text": "References\n\n\nChandra, K., Chen, T., Tenenbaum, J. B., & Ragan-Kelley, J. (2025). A domain-specific probabilistic programming language for reasoning about reasoning (or: A memo on memo). Proc. ACM Program. Lang., 9(OOPSLA2). https://doi.org/10.1145/3763078\n\n\nXiang, Y., Vélez, N., & Gershman, S. J. (2023). Collaborative decision making is grounded in representations of other people’s competence and effort. J. Exp. Psychol. Gen., 152(6), 1565–1579."
  }
]