[
  {
    "objectID": "presentation/index.html#how-do-we-judge-what-others-could-bring-to-the-table",
    "href": "presentation/index.html#how-do-we-judge-what-others-could-bring-to-the-table",
    "title": "Thinking About Collaborators",
    "section": "How do we judge what others could bring to the table?",
    "text": "How do we judge what others could bring to the table?\n\n\nWe observe them work, to update our beliefs! (Bayesian)\n\nBelief: how competent they are (cite)\nBelief: how much effort they are putting in (ToM), given their competence (cite) and how much incentive there is (cite)\n\nWe make assumptions:\n\nIf they succeed, they must have some competence and have put some effort in\nIf they are competent enough to succeed, they will allocate the minimum effort needed given their competence\nEffort is costly, so if they succeed with low incentive, they must really desire the reward (i.e., reward has high utility to them)\n\n\n\nCollaboration is key to human success. When we think about other people we could work with, we need to judge who’s capable enough to be a good potential collaborator. So how do we make these judgments?\nWe observe them work to update our beliefs; Bayesian inference is a good candidate for how we adjust our beliefs given observations of the world\nWe can make these inferences tractable by making some assumptions:"
  },
  {
    "objectID": "presentation/index.html#when-working-together-how-do-we-calibrate-our-own-effort",
    "href": "presentation/index.html#when-working-together-how-do-we-calibrate-our-own-effort",
    "title": "Thinking About Collaborators",
    "section": "When working together, how do we calibrate our own effort?",
    "text": "When working together, how do we calibrate our own effort?\n\nSimple models:\n\nWe simply think the other person won’t apply any effort, so we allocate effort like we are working alone (“solitary” model)\nWe simply think the other person will apply maximum effort, so we allocate only enough effort to help them (“compensatory” model)\n\nProposed model in Xiang et al. (2023):\n\nIf we think they can succeed, we jointly optimize our effort: we recursively infer how much effort the other will allocate, and adjust our own effort accordingly, while also trying to be fair (“joint effort” model)\n\n\n\nAfter we infer each other’s competence from observing their behavior, how do we set our own effort when collaborating with them?"
  },
  {
    "objectID": "presentation/index.html#xiang2023-zo-lift-that-box",
    "href": "presentation/index.html#xiang2023-zo-lift-that-box",
    "title": "Thinking About Collaborators",
    "section": "Xiang et al. (2023): Lift That Box!",
    "text": "Xiang et al. (2023): Lift That Box!\n\nXiang et al. (2023) used a game-like task to collect participants’ judgments of competence, effort, and likelihood of success, to test the “joint effort” model \n\n\nThe original study used this task which looks like… (demo… mention joint and individual activity)"
  },
  {
    "objectID": "presentation/index.html#xiang2023-zo-lift-that-box-1",
    "href": "presentation/index.html#xiang2023-zo-lift-that-box-1",
    "title": "Thinking About Collaborators",
    "section": "Xiang et al. (2023): Lift That Box!",
    "text": "Xiang et al. (2023): Lift That Box!\n\nSome critiques:\n\nThe task is not very naturalistic (limits ecological validity)\nThe participant isn’t a collaborator themselves (limits construct validity)"
  },
  {
    "objectID": "presentation/index.html#xiang2023-zo-result",
    "href": "presentation/index.html#xiang2023-zo-result",
    "title": "Thinking About Collaborators",
    "section": "Xiang et al. (2023): Result",
    "text": "Xiang et al. (2023): Result\n\nPs thought that collaborators could achieve a task jointly even if they have previously failed the task individually (t(49) = 10.4200808, p = 5^{-14}; d = 1.47)\nPs thought that collaborators’ chance of jointly achieving the task gradually increased as collaborators succeeded more individually (i.e., that collaborators thought about each other’s effort and tried to distribute effort fairly)\n\nTheir proposed “joint effort” model was the only one that explained both effects\n \n\n\ncan note that there’s a green dot as well showing maximal model, but it’s not relevant to us - it’s another alternative model that didn’t give predictions for most scenarios"
  },
  {
    "objectID": "presentation/index.html#replicating-xiang2023-zo",
    "href": "presentation/index.html#replicating-xiang2023-zo",
    "title": "Thinking About Collaborators",
    "section": "Replicating Xiang et al. (2023)",
    "text": "Replicating Xiang et al. (2023)\n\nI estimated that we needed only 9 participants (as this would give us 97.0458054% power for the main effect)\nHowever, to replicate the qualitative pattern, in the absence of a good expectation, I wanted the full 50 participants"
  },
  {
    "objectID": "presentation/index.html#replicating-xiang2023-zo-statistics-and-modeling",
    "href": "presentation/index.html#replicating-xiang2023-zo-statistics-and-modeling",
    "title": "Thinking About Collaborators",
    "section": "Replicating Xiang et al. (2023): Statistics and Modeling",
    "text": "Replicating Xiang et al. (2023): Statistics and Modeling\n\nI expected the basic effect to replicate\nFor the computational model, I chose to reimplement the model in a new probabilistic programming language (memo) for performance and extensibility advantages (for future work)\n\nNeeded to validate that new model had comparable results\nWorked through distinguishing trivial implementation differences (e.g., going from a continuous prior to a discrete prior) from meaningful implementation discrepancies (e.g., errors in the model implementation)\nOvercame this by changing one implementation variable at a time and measuring its impact to characterize where discrepancies came from (to qualify how they looked numerically)\n\n\n\nI expected the basic effect to replicate since there’s high signal to noise in the measure (high t-value and low SE), and I was 4/5 confident the qualitative pattern would replicate.\n(how I changed one variable at a time in modeling: webppl MCMC -&gt; webppl enumerate -&gt; memo enumerate)"
  },
  {
    "objectID": "presentation/index.html#replication-results",
    "href": "presentation/index.html#replication-results",
    "title": "Thinking About Collaborators",
    "section": "Replication Results",
    "text": "Replication Results\n\n\n\n\n\n\n\nOriginal\nReplication\n\n\n\n\n“Note. … Error bars indicate bootstrapped 95% confidence intervals.” (p. 1571)\nNote. Error bars indicate bootstrapped 95% confidence intervals.\n\n\nCollaborators could achieve a task jointly even if they have previously failed the task individually (t(49) = 10.4200808, p = 5^{-14}; d = 1.47)\nReplicated! (t(49) = 9.2641181, p = 2.4^{-12}; d = 1.31)\n\n\n\n\nReplicated: joint success is possible even if there is no prior evidence of success"
  },
  {
    "objectID": "presentation/index.html#replication-results-1",
    "href": "presentation/index.html#replication-results-1",
    "title": "Thinking About Collaborators",
    "section": "Replication Results",
    "text": "Replication Results\n\n\n\n\n\n\n\n\nOriginal\nReplication\n\n\n\n\n “Note. … Model simulations averaged over 10 runs. Error bars indicate bootstrapped 95% confidence intervals.” (p. 1571)\n Note. Model simulations consider strengths from 1 to 10 (inclusive) in steps of 0.03 (determined by memory constraints). Error bars indicate bootstrapped 95% confidence intervals.\n\n\nJoint success likelihood increases as cases of individual success increases\nReplicated!\n\n\n\n\nReplicated: joint success is possible even if there is no prior evidence of success, AND joint success likelihood increases as cases of individual success increases\n\nIn conjunction, these support the joint-effort model of reasoning about collaborators’ competence and effort."
  },
  {
    "objectID": "presentation/index.html#discussion",
    "href": "presentation/index.html#discussion",
    "title": "Thinking About Collaborators",
    "section": "Discussion",
    "text": "Discussion\n\nReplication success was expected due to large effect size and clear pattern\nI observed slight differences:\n\nlarger CIs\na flat section of the curve (i.e. similarity between two scenarios) not seen before; could be explained by the safe joint effort model (can show the figure that includes the safe joint effort model)\nEffect size (d) was lower than original, as should be expected\n\n\n\nMaybe the larger CIs are due to more variance than in the original?"
  },
  {
    "objectID": "presentation/index.html#discussion-1",
    "href": "presentation/index.html#discussion-1",
    "title": "Thinking About Collaborators",
    "section": "Discussion",
    "text": "Discussion\n\nTo test the hypotheses further, I might model within-subject variability independent from scenario condition\n\n\n\nMaybe the greater variance in the data (driving the larger CI values) is due to this heterogeneity that might not have been present in the original data"
  },
  {
    "objectID": "presentation/index.html#references",
    "href": "presentation/index.html#references",
    "title": "Thinking About Collaborators",
    "section": "References",
    "text": "References\n\n\nXiang, Y., Vélez, N., & Gershman, S. J. (2023). Collaborative decision making is grounded in representations of other people’s competence and effort. J. Exp. Psychol. Gen., 152(6), 1565–1579."
  },
  {
    "objectID": "writeup/index.html",
    "href": "writeup/index.html",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "",
    "text": "My broad research interests include modeling how individuals perceive, represent, and judge potential collaborators’ affordances for collaboration. Xiang et al. (2023) propose and empirically validate a probabilistic model in which recursively updated inferences of competence and effort (“belief–desire–competence framework”), together with observed outcomes, jointly predict individuals’ judgments of collaborators, across various cognitive tasks which are common within collaboration. In particular, in Experiment 1, their model best predicts participants’ judgments about whether joint activity will succeed, compared to plausible alternative models. I am eager to attempt to replicate this finding to help establish the robustness of this model of collaborator judgment, before I may attempt to extend it in future work.\nExperiment 1 involves observing contestants try to lift a heavy box in a set of contests (six total), each containing three rounds. In each contest, there are two unique avatars playing as the contestants. In rounds 1 and 2, each contestant attempts to lift the box by themselves. In round 3, the contestants attempt to lift the box together. In each round, after observing each contestant succeed or fail to lift the box, the participant judges the strength of each contestant (1–10), and for each contestant that successfully lifted the box, the participant judges that contestant’s allocated effort (0%–100%). At the start of round 2, the participant judges each contestant’s probability of lifting the box successfully (0%–100%), prior to observing the lift attempts. At the start of round 3, the participant judges the probability that the contestants will successfully lift the box together. Further, an incentive to the contestants for lifting the box is specified in each round: in the second and third rounds, the specified reward for lifting the box is double that of the first round. Participants are also told that the heavy box had a constant weight across all contests, which effectively requires at least 5 strength points applied (e.g., on average, 100% effort for a contestant with strength 5, or 50% effort for a contestant with strength 10, etc.). Last, participants see a progressively filled-out table of all the lift outcomes during each contest, visible throughout, including when making judgments.\n\n\n\nExample illustrations shown during instructions; note that the incentive displayed for round 1 is $10, not $20.\n\n\n\n\n\nAn example table for a contest, as it would appear during the probability judgment at the beginning of round 3.\n\n\nThe challenges/opportunities for my learning in this replication project will include: 1) recreating the paper’s proposed (joint effort) model and two alternative (solitary/compensatory effort) models of the probability of joint success in memo (based on the authors’ pre-existing implementations in WebPPL), and 2) running these models over my newly collected data to attempt to replicate that the proposed model is most predictive of participants’ judgments. (I will also replace the original authors’ data-saving step (a php page) with DataPipe.)\nSpecifically, I will attempt to replicate that A) when rounds 1 and 2 feature no individual success, participants still judge the probability of joint success in round 3 as non-zero. In addition to this confirmatory analysis, I will attempt to qualitatively replicate that B) participants judge higher probability of joint success as cases of individual success increases, and that the proposed model is the only model evaluated which predicts both patterns A and B. See Analysis Plan below for more details.\n\n\n\n\n\n\n\nNoteLinks\n\n\n\nRepository: https://github.com/psyc-201/xiang2023\nOriginal paper: 2023_xiang_effort.pdf (Retrieved on Sep 30, 2025 from velezlab.org)\nHosted experiment: https://psyc-201.github.io/xiang2023/exp/index1.html\nHosted version of this report: https://psyc-201.github.io/xiang2023/writeup/"
  },
  {
    "objectID": "writeup/index.html#introduction",
    "href": "writeup/index.html#introduction",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "",
    "text": "My broad research interests include modeling how individuals perceive, represent, and judge potential collaborators’ affordances for collaboration. Xiang et al. (2023) propose and empirically validate a probabilistic model in which recursively updated inferences of competence and effort (“belief–desire–competence framework”), together with observed outcomes, jointly predict individuals’ judgments of collaborators, across various cognitive tasks which are common within collaboration. In particular, in Experiment 1, their model best predicts participants’ judgments about whether joint activity will succeed, compared to plausible alternative models. I am eager to attempt to replicate this finding to help establish the robustness of this model of collaborator judgment, before I may attempt to extend it in future work.\nExperiment 1 involves observing contestants try to lift a heavy box in a set of contests (six total), each containing three rounds. In each contest, there are two unique avatars playing as the contestants. In rounds 1 and 2, each contestant attempts to lift the box by themselves. In round 3, the contestants attempt to lift the box together. In each round, after observing each contestant succeed or fail to lift the box, the participant judges the strength of each contestant (1–10), and for each contestant that successfully lifted the box, the participant judges that contestant’s allocated effort (0%–100%). At the start of round 2, the participant judges each contestant’s probability of lifting the box successfully (0%–100%), prior to observing the lift attempts. At the start of round 3, the participant judges the probability that the contestants will successfully lift the box together. Further, an incentive to the contestants for lifting the box is specified in each round: in the second and third rounds, the specified reward for lifting the box is double that of the first round. Participants are also told that the heavy box had a constant weight across all contests, which effectively requires at least 5 strength points applied (e.g., on average, 100% effort for a contestant with strength 5, or 50% effort for a contestant with strength 10, etc.). Last, participants see a progressively filled-out table of all the lift outcomes during each contest, visible throughout, including when making judgments.\n\n\n\nExample illustrations shown during instructions; note that the incentive displayed for round 1 is $10, not $20.\n\n\n\n\n\nAn example table for a contest, as it would appear during the probability judgment at the beginning of round 3.\n\n\nThe challenges/opportunities for my learning in this replication project will include: 1) recreating the paper’s proposed (joint effort) model and two alternative (solitary/compensatory effort) models of the probability of joint success in memo (based on the authors’ pre-existing implementations in WebPPL), and 2) running these models over my newly collected data to attempt to replicate that the proposed model is most predictive of participants’ judgments. (I will also replace the original authors’ data-saving step (a php page) with DataPipe.)\nSpecifically, I will attempt to replicate that A) when rounds 1 and 2 feature no individual success, participants still judge the probability of joint success in round 3 as non-zero. In addition to this confirmatory analysis, I will attempt to qualitatively replicate that B) participants judge higher probability of joint success as cases of individual success increases, and that the proposed model is the only model evaluated which predicts both patterns A and B. See Analysis Plan below for more details.\n\n\n\n\n\n\n\nNoteLinks\n\n\n\nRepository: https://github.com/psyc-201/xiang2023\nOriginal paper: 2023_xiang_effort.pdf (Retrieved on Sep 30, 2025 from velezlab.org)\nHosted experiment: https://psyc-201.github.io/xiang2023/exp/index1.html\nHosted version of this report: https://psyc-201.github.io/xiang2023/writeup/"
  },
  {
    "objectID": "writeup/index.html#methods",
    "href": "writeup/index.html#methods",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "Methods",
    "text": "Methods\n\nPower & Precision Analysis\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe original effect size is d = 1.47.  To achieve 80%, 90%, or 95% power to detect that effect size, I’d need:\n\n\n\nTarget Power\nNecessary N\nEstimated Power\n\n\n\n\n80%\n6\n81.9866034%\n\n\n90%\n8\n94.4324728%\n\n\n95%\n9\n97.0458054%\n\n\n\nCollecting 9 participants is completely feasible (with each spending ~16 minutes; see Procedure). To compensate for the original study’s measured effect size being inflated (e.g., due to publication bias), 15 participants should be necessary for sufficient power for the primary effect of interest, and this count is also feasible.\nHowever, for the sake of qualitatively replicating the overall pattern of estimated probabilities increasing as scenarios feature more successful individual lifts (general increase in probability from left to right in Figure 3C; which I’ve labeled “B” above), and that the joint effort model is the only considered model which captures both the key effect and this pattern, I find it prudent to retain the same sample size so that I may attempt to replicate the original qualitative finding with comparable precision. (Targeting higher precision could potentially counter aforementioned effect inflation present in the original paper, but as it is unclear how to quantify the necessary precision for these qualitative analyses, targeting the original precision appears to be a sensible option.)\nFurther, in the original study, given 50 participants, the key effect appeared quite robust, with the CI for the data from scenario “F,F;F,F” being quite far from the line LiftProbability=0 (see Figure 3C), suggesting that with 50 participants, we are nearly guaranteed to replicate this effect if it is true.\nGiven my understanding of course funding available, collecting 50 participants is feasible.\n\n\nPlanned Sample\nBased on my power and precision considerations, I will collect 50 participants on Prolific (from the U.S., per the original authors’ private note), following the specific exclusion strategy reported below.\nI will follow precisely the data collection strategy in the original Experiment 1, as described on p. 1569:\n\n“Participants’ demographic information was not collected. Participants completed a comprehension check before they moved on to the experiment. They were not allowed to proceed until they answered all the comprehension check questions correctly. … To ensure data quality, we included two attention-check questions in the experiment. Participants who failed one attention check were warned immediately to pay closer attention. Participants who failed both attention checks were asked to leave the experiment and they were not counted among the 50 participants we recruited. A total of 10 participants failed one attention check, and we did not exclude their data in our analysis.”\n\nIn the original study, per the experiment code, participants who fail both attention checks are asked to return their submission (to self-exclude). Per our current IRB approval, we are not asking these participants to do so and will instead manually exclude them from our target sample and analysis.\n\n\nMaterials\nI will follow precisely the original Experiment 1. The materials for the experiment are available at https://github.com/jczimm/competence_effort. This repository is a clone of the authors’ own code repository for the paper, with the experimental task code copied from the authors’ hosted version linked in their README.md file.\nSee Procedure below for more details and a figure from the paper visualizing the task.\n\n\nProcedure\nI will follow precisely the original Experiment 1, as described on pp. 1569-1570:\n\n“… participants provided informed consent prior to the experiment. … Participants observed six contests between different pairs of contestants (see Table 2 for a description of the contests; the order was randomized). In each contest, the contestants were given three attempts to lift a box, corresponding to three rounds. In the first two rounds, the contestants tried lifting the box themselves. The reward for lifting the box was $10 in Round 1 and $20 in Round 2. In the third round of each contest, the two contestants tried to lift thebox together for a reward of $20 each. Participants first saw the lift outcome of Round 1 and made strength judgments (1–10; 1 means extremely weak and 10 means extremely strong) and effort judgments (0%–100%) for each contestant. For Rounds 2 and 3, they predicted the probability of the contestants lifting the box (0%–100%) before seeing the outcome, then observed the actual outcome and made strength and effort judgments. Note that participants made effort judgments only when the outcome was Lift. Participants were informed that the weight of the box was always the same and equivalent to a strength of 5 (i.e., an average contestant with strength 5 exerting all of their efforts would be able to lift the box). Participants also saw a table of all the previous outcomes when making their guesses. Figure 2 shows an illustration of the task.”  \n\nTo summarise: there are six contests, each with three rounds, and multiple judgments during each round.\nAccording to the consent form in the original experiment, the study is estimated to take 15 minutes. Accordingly, adhering to Prolific’s recommended rate of compensation ($12/hr), base compensation will be $3.\nFor determining bonus compensation, I will follow precisely the original scheme as described on p. 1569:\n\n“Participants received … a potential bonus payment of up to $1. The amount of bonus they received was equal to the probability they put on the realized lift outcome on a randomly picked round.”\n\n\n\nAnalysis Plan\nI will conduct a one-sample t test on the participants’ reported round-3 lift probability for scenario “F,F;F,F” (i.e., each contestant failed to lift the box in both rounds 2 and 3), with the null hypothesis that the true lift probability is zero. See Planned Sample for the data exclusion rule.\nIn addition to this confirmatory analysis, I will attempt to replicate the qualitative pattern that participants judge higher probability of joint success as cases of individual success increases. I will also reimplement their proposed model (joint effort) and alternative models (solitary effort and compensatory effort) in memo (see Differences from Original Study) and attempt to replicate that the proposed model is the only model evaluated which predicts both the qualitative pattern and the other result (that participants estimate round-3 lift probability as non-zero after two rounds of failures). See original paper’s Figure 3C and accompanying explanation, pp. 1570-1571; I will recreate that figure and attempt to replicate that explanation on my own data. Further, I will also estimate and statistically test the difference in the probability judgments plotted in Figure 3C between the original data and my new data.\nI must note that while the t test alone does not justify the paper’s central claim that the joint effort model is qualitatively predictive, in the absence one such statistical test, this t test is a good alternative: this test demonstrates one of two key behavioral effects (p. 1570) which the model was qualitatively evaluated to predict. (In other words, I should attempt to replicate this behavioral effect before I could try to qualitatively replicate that the joint effort model is the only considered model which can predict it.)\n\n\nDifferences from Original Study\nThe sample will differ in that the original was from Amazon Mechanical Turk, while the new sample will be from Prolific. Accordingly, base compensation will be $3 instead of $2.\nThe only difference in setting is that the task will be hosted at https://psyc-201.github.io/xiang2023/exp/index1.html rather than https://gershmanlab.com/experiments/yang/toc/Experiment/index1.html.\nThe only known visible differences in procedure are:\n\nThe consent form is updated according to the details provided by the UCSD course PSYC 201A\nIn task instructions, “HIT” is renamed “submission”\nIn task instructions, “Different pairs of contestants will come in to lift the box” becomes “For each contest, a new pair of contestants will come in to lift the box” (to clarify first two questions of comprehension check)\nNo more than two attempts of the comprehension check will be allowed (per Prolific guidelines). Accordingly, before the comprehension check, we added: “You will have two chances to complete the comprehension questions correctly before we ask you to return your submission, per Prolific guidelines.”; and upon comprehension check failure, we added: “Please note that you have only X attempt(s) left.”\n\nWhile the confirmatory analysis will be rerun using the original R code provided by the authors, for the qualitative replication attempt I will use a reimplementation of the joint effort model and the alternative models (solitary effort and compensatory effort models) in memo, which were originally written in WebPPL. I expect that reimplementation of the continuous probabilistic models in WebPPL as discrete probabilistic models in memo will produce higher estimate precision as the model predictions will be deterministic, rather than stochastic. (I will discretize the distribution at a high resolution such that there is no systematic difference in estimates.)\nI do not anticipate that these differences are meaningful regarding the original paper’s claims and the analyses of interest. However, it is possible that 3 and 4 - improving comprehension-check fairness and reducing testing fatigue - may slightly improve measurement precision and reduce measurement bias, enhancing the claims of my replication.\n\n\nReliability and Validity\nThe key measure is the participant’s report of the probability of the contestants successfully lifting the box in round 3. For this measure, the latent construct of interest is the participant’s internal estimate of lift probability.\nThe reliability of this measure is unclear, given a lack of referenced evidence of the reliability of an explicit probability judgment. However, in the current model of the participant’s judgment as Bayesian inference, I infer that the reliability is negatively correlated with the noisiness of the participant’s internal translation from implicit posterior distribution (over lift probability) to explicit report; in other words, this measure can be no more reliable than the reliability of participants’ own estimation of the expected value of their internal posterior distribution. As there may also be individual differences in the noisiness of this translation process reliability could be reduced by this.\nLikewise, validity is unclear. And following the same logic, I infer that the validity may vary across participants, given that there could be individual differences in the accuracy of participations’ estimation of the expected value of their internal posterior distribution."
  },
  {
    "objectID": "writeup/index.html#results",
    "href": "writeup/index.html#results",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "Results",
    "text": "Results\n\nData preparation\nThe data collected in this experiment will include task-level data:\n\nsubjectId, encryptedProlificId\npre-set round outcomes (round1_o, round2_o, round3_o)\npre-set round incentives (round1_reward, round2_reward, round3_reward)\nnumber of failed attention checks (attention_sum)\n\nIt will also include round-level data:\n\ncontest index (contest)\nround index (round)\nstrength (r1_strength_a/r2_strength_a/r3_strength_a, and r1_strength_b/r2_strength_b/r3_strength_b)\neffort (r1_effort_a/r2_effort_a/r3_effort_a, and r1_effort_b/r2_effort_b/r3_effort_b)\noutcomes (r1_outcome_a/r2_outcome_a/r3_outcome_a, and r1_outcome_b/r2_outcome_b/r3_outcome_b)\nprobability (r1_prob_a/r2_prob_a/r3_prob_a, and r1_prob_b/r2_prob_b/r3_prob_b)\n\nWe will manually exclude any participants who failed both attention checks.\n\n\nConfirmatory analysis\nI found that participants rated the probability of joint success in round 3 as non-zero (t(49) = 9.2641181, p = 2.4^{-12}). See the black point range (data) for scenario “F,F;F,F” under Exploratory analyses.\nThe effect size found is d = 1.31.\n\n\n\n\n\n\n\n\n\nThe red line shows the critical t-value for detecting a statistically significant effect with α = 0.05.\n\n\nExploratory analyses\nI found/did not find that the joint effort model was the only model which qualitatively predicted both that joint success has non-zero probability (the result in confirmatory analysis) and that probability of joint success increases as individual success increases.\n\n\n\n\n\n\n\nOriginal (Figure 3C)\nNew (Figure 3C Replication)\n\n\n\n\n“Note. … Model simulations averaged over 10 runs. Error bars indicate bootstrapped 95% confidence intervals.” (p. 1571)\nNote. Model simulations consider strengths from 1 to 10 (inclusive) in steps of 0.03 (determined by memory constraints). Error bars indicate bootstrapped 95% confidence intervals.\n\n\n\n\n\n\n\n\n\nNoteFigure 3C Replication with Raw Data\n\n\n\n\n\n\n\n\nFigure 3C Replication with Raw Data\n\n\n\n\n\n\n\n\n\n\n\nNoteFigure 3C Replication with Safe Joint Model\n\n\n\n\n\n\n\n\nFigure 3C Replication with Safe Joint Model"
  },
  {
    "objectID": "writeup/index.html#references",
    "href": "writeup/index.html#references",
    "title": "Replication of Experiment 1 by Xiang, Vélez & Gershman (2023, JEP:G)",
    "section": "References",
    "text": "References\n\n\nXiang, Y., Vélez, N., & Gershman, S. J. (2023). Collaborative decision making is grounded in representations of other people’s competence and effort. J. Exp. Psychol. Gen., 152(6), 1565–1579."
  }
]